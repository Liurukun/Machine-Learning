---
title: "Lesson 3: Deep Learning as Data-driven Methods"
description: |
  The immediate aim of this section is to develop a data-driven machine learning algorithm to predict which interactions are missing from ecological networks, and to explore ways in which ecological insight can be extracted from the algorithm. In particular, we introduce key concepts and terminology of data-driven models.
#author: []
# author: []
# date: 2023-10-15
# output:
#   distill::distill_article:
#     self_contained: false

date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Ecology and evolutionary biology investigate complex patterns and processes. A mathematical toolkit has been necessary to describe and explain fundamental components of organic evolution and ecological interactions. This wealth of data is driving the development of analytic tools that can provide new understanding, greater efficiency, and ease of use. Likelihood-based mechanistic approaches designed to consider many variables can be so computationally expensive that they can no longer be applied to data routinely generated in modern studies. A promising alternative is likelihood-free inference, one example of which is machine learning. The goal of machine learning is to find a model that performs well at making predictions from the data. This contrasts with likelihood-based methods, which assume the model generating the data is known.  More recently machine learning has seen a dramatic surge in popularity
with a slew of new algorithms and applications.

One of the approaches rapidly gaining popularity is deep learning. Deep learning relies on multilayered, connected processing units (artificial neural networks or ANNs). The successes of deep learning were possible because of a major advantage of deep learning over classical machine learning approaches. Classical machine learning requires that important data features are identified using expert domain knowledge. Neural networks can automatically discover the most important data features and patterns relevant for the task. Researchers are now beginning to apply deep learning to problems across ecology and evolutionary biology, from community science projects and environmental monitoring through sequencing equipment output processing, to population genetics
and phylogenetic inference. 

## 1. What are neural networks and how do they learn?

Here describe what [artificial neural networks](https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network) are and how they are used as inference tools (Box 1).<br>

<div class="warning" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>Box 1: Common neural network architectures</b></p>
<p style='margin-left:1em;'>
Artificial Neural Networks(ANNs) is a group of multiple perceptrons/ neurons at each layer. Neurons can be connected to other neurons in different layers but not within the same layer. In the simplest form, a neural network has one input layer, at least one intermediate or hidden layer, and an output layer. The trainable network parameters consist of biases and weights. Each node has a bias value (b), which determine how easy it is for it to “fire”. Each connection has a weight value (W) which represents connection strength. Each node also has an activation function (f), for example sigmoid
function. Given some input (x), the so-called feedforward output (y) of a node is determined by a simple equation: $y = f(W × x + b)$.

ANN is a Feed-Forward Neural network because inputs are processed only in the forward direction. Recurrent neural networks is to add loops to information flow. Information flows from the input to the output of the network but it can flow back from the output to the input of the hidden layer through recurrent weights. 

<center>
![](ANN_RNN.png)
</center><br>
However, simple RNNs such as the one shown in the figure are difficult to train because weights in these networks can quickly diverge during training. More advanced RNN, such as Long Short-Term Memory networks (LSTMs) or Gated Recurrent Units (GRUs), address this problem and are commonly used with time series data. In evolutionary biology, deep learning solutions including GRU have been used to predict recombination landscapes. In the field of ecology. reserchers are trying to use deep learning techonologies for the analysis of remote sensing data. The convolutional neural networks (CNNs) can excel at capturing complex, hierarchical patterns and are the architecture used in most identification and classification problems.<br>
</p>
<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> 
</p></span>
</div>
<br>

In mathematical sense, neural networks are simply a function mapping input onto a desired output. This general design is simple, but it makes neural networks
extraordinarily powerful: a network with information flowing from input to output layer with at least one intermediate layer (i.e. feedforward network) can approximate any continuous function, regardless of its complexity. These approximations can describe pixels of an image, for example, and networks with multiple intermediate layers (deep neural networks) can also learn relationships between them as high-level concepts such as lines, geometric shapes, and even whole scenes. 

ANNs learn continuous distributions but their output can represent probabilities of distinct data classes, as well as continuous values. Such networks can thus be used to construct classifiers, which are models distinguishing among discrete categories, as well as regression models, which infer continuous values. But feedforward operations alone do not allow the network to learn or generalize to new data, which is the essence of most ANN applications.

In order for an ANN to be a predictive tool, one needs to assess how good predictions are and be able to adjust ANN parameters to improve performance. A measure of how far off the output of the network is is called a loss function. One example of a loss
function is the sum of squares error (SSE), which is simply the sum of differences between each predicted value (y) and the true value ($\overline{y}$) squared for absolute value:

$$SEE= \sum_{i = 1}^{n}(y_i - \overline{y}_i)^2$$

The network also needs a mechanism for finding the set of parameters that minimize the loss function. Once the loss (error) is measured at the output, it has to be traced back across the network to measure how parameters contributed to it. This process is called backpropagation and it uses chain rule calculus to find the derivative (slope) of the loss function with respect to the network’s trainable parameters. The process of increasing or decreasing parameters such that they minimize the derivative of the loss function is called gradient descent. This process is iterative, occurring every time a batch of training data is processed, and collectively referred to as the training loop. When devised correctly, it results in improvement of inference accuracy with each pass of the loop.

The fact that ANNs are universal approximators for continuous functions that can be trained makes them powerful predictive tools. This learning scheme is most easily illustrated with an example of supervised learning, where the network is trained on a data set, e.g. images of expert-identified species or methylated vs. unmethylated DNA sequences. Supervised training usually involves splitting data into three subsets:
training, validation, and testing. The validation set is not directly used in training but prediction on it is performed at the end of each training cycle to assess how well the network generalizes outside of the training set. Test set is held back for the final estimate of accuracy. 

## 2. Appplications of deep learning to ecology and evolution
In the sections that follow, we review how deep learning has been applied in ecology and evolution, including species identification and monitoring, ecological and behavioral studies, and population genetics and phylogenetics. We use these examples to showcase the variety of deep learning techniques extending its usage beyond the general picture described above.

### 2.1 Automated species identification
Deep learning enabled breakthroughs in automated image classification, largely possible thanks to CNNs. Image recognition has obvious applications in biology and was
adopted early for problems of species identification and wildlife monitoring. It is not surprising then that identification or classification of individuals or species from image, video, and sound data is the most common use of deep learning in the field. These efforts already span many taxa, from bacteria, through protozoans, plants to insects and vertebrates, both extant and fossil and at scales ranging from local to global. Intensifying efforts to digitize natural history collections provide troves of
image data that can be used for this purpose. 

Camera trap systems and deep learning classifiers are now commonly used for vertebrate wildlife monitoring and systems automating environmental monitoring of aquatic macroinvertebrates are also being developed. Many publications present systems or deep learning models for detecting and identifying pests or crop diseases in agroecosystems or stored agricultural
commodities. Despite economic importance and demonstrated potential for crop pest and disease monitoring, at present few non-proprietary systems or open source software
applications exist. A notable exception is a mobile application system for identifying diseases of cassava plants, one of the most important tropical crops. 

Deep learning has also been applied to identification from audio recordings, including bird and bat sounds, and even wing beats of mosquitoes. Unsurprisingly, the technology has been applied most often to bird calls, where it has been used not only to identify species, but also monitor their abundance. The recently developed BirdNET is a deep neural network capable of identifying North American and European birds from vocalizations in complex soundscapes, available on a variety of platforms, including user-friendly smartphone apps. Most of these studies use audio converted to spectrograms, image representations of sound, to train CNNs as in visual recognition problems.

Given its utility for automated identification, deep learning is increasingly used in community science initiatives. Examples include a growing number of mobile phone applications such as plant-focused identification app Pl@ntNet, bird identification tool Merlin or the citizen naturalist portal iNaturalist, as well as a number of more local or taxon-specific guides. Many of these applications crowd-source training data collection and identification verification by users. They improve by periodically re-training their deep learning classifiers as more reliable data is collected.

Many of these studies employ data handling approaches that increase performance of deep learning classifiers. Several use data augmentation, a technique that relies on altering training data with distortion. These modifications, applied to each data input in each training epoch, effectively increase training set size. Data augmentation is an important strategy for reducing overfitting and almost always results in increased classifier accuracy. By ensuring that the neural network never sees the same input twice, augmentation only partly addresses the fact that acquiring large, human-labeled datasets is a bottleneck for many applications. An alternative approach is to train an initial classifier in a supervised way, using a labeled training set, and then use this reasonably well-performing classifier for adding more images in an unsupervised manner, without human intervention. 

Another technique ubiquitous in identification and classification tasks is transfer learning. Transfer learning is most commonly accomplished by first training on a different, usually larger and more general dataset than the one assembled for the problem on hand. The resulting network parameters can then be used as the starting point for fine-tuning on the focal dataset. In species recognition from images it is common to use networks pre-trained on large, public datasets of everyday objects such as ImageNet or COCO as illustrated by several of the studies cited above. Using pre-trained networks makes the network learn faster and often results in higher accuracy.

In addition to properly assigning a label to an image, termed image classification, a common computer vision problem is to localize objects. Object recognition is a term often used for the combination of the two: drawing a bounding box around an object and predicting its class. Because there may be many objects in an image, this is a more challenging problem. The many proposed solutions involve either extracting candidate regions from images prior to prediction or predicting classes directly on grids of image pixels. Examples are common in agriculture, where object detection has been used to identify and count pests. 

The deep learning framework allows training several neural networks of the same or varying architectures on one dataset and averaging their predictions. Known as model ensembling, this technique reduces variance in predictions and can improve accuracy. Examples in species identification include Finnish fungi recognition and UK ladybird beetles. 

Finally, deep learning is not limited to considering image pixels alone but can also take advantage of contextual information such as locality or phenology. For example, output can be improved by filtering out
nonsensical predictions given prior occurrence data. This approach, however, does not jointly consider the available data in a common framework. Neural networks can be trained on multiple data inputs simultaneously and consider them jointly in the final layers. One study used this approach for beetle identification from images and found improvement in accuracy with information about location, date, weather, habitat, and
user expertise.

### 2.2 Environmental monitoring and modeling

The above mentioned approaches to automated identification of species or individuals are also being scaled to ecosystem scale and applied to diversity assessment, conservation, and resource management. Examples using techniques detailed
in the previous section include detecting and estimating abundance of zooplankton
and detecting and counting sea turtles and whales using drone and satellite imagery. Other uses combine digital imagery with LiDAR and other remote sensing or geospatial data for mapping of vegetation, forest carbon stock, and the footprint of fishing across the world’s oceans. Similar applications include integrated systems for real-time wildlife monitoring using data from camera traps and microphone and raise the prospect of surveillance of social media posts for illegal animal trade.

In addition to classification and mapping of static information, RNNs and similar approaches have been used with temporal ecological data. Examples include predicting
eutrophication, phytoplankton blooms, and benthic invertebrate community dynamics. As mentioned previously, combining inputs from different sources is natural for deep learning and Rammer and Seidl take advantage of this to predict and map future bark beetle outbreaks based on temporal information on climate, vegetation, and past outbreaks. Capinha et al. proposed a generalized approach to classification and prediction from ecological time series data leveraging automated choice of the best network architecture for the task at hand.

Finally, neural networks are being used to develop more realistic models and simulations of real world patterns and phenomena. Benkendorf and Hawkins found that deep neural networks can be used to generate accurate species distribution models but
also noted that other machine learning approaches perform as well or better with limited training data. Strydom et al. designed a system to predict species interactions from co-occurrence data. A study using reinforcement learning investigated how learning to hunt or avoid predators by individual agents influenced predator-prey dynamics.

### 2.3 Behavioral studies

The study of animal behavior, both in the field and controlled laboratory settings, is another research area of ecology and evolution that is poised to greatly benefit from adoption of deep learning. Recent technological advancements in sensing, monitoring, and automation allow behavioral ecologists to collect and analyze large amounts of data Long-standing challenges in identifying, quantifying, and analyzing animal behavior still limit the ability to fully automate processing of these data, however. Deep learning has the potential to address many of these challenges and it is increasingly being adopted in studies involving identification of individual animals, body posture and movement tracking, and classification of behaviors.

In the area of animal body posture, deep learning can provide non-invasive estimation of the position of animals’ body parts from video recordings. Several open-source toolkits have been developed for this purpose, ranging from species-specific solutions, to generic frameworks applicable to any species, some of which offer 3-dimensional and/or multiple animals tracking. In addition to pose estimation,
deep learning is also being adopted to enhance the performance of established computer vision methods used to track spatial position of animal detection or the identification of markers, as well as to automatically perform behavioral analysis of spatial trajectories.

Deep learning can also allow for the identification, classification, and subsequent re-identification of individual animals from camera feeds or traps, both in the wild and in captivity. Usually based on the use of CNNs for image recognition, deep
learning can also be combined with other technologies to develop automated
data-processing pipelines to collect and label samples bird species. A popular application in this area is face recognition enabling mark-recapture studies for monitoring populations of individuals, their behavior, and social interactions. Examples in the wild include identification of elephants, chimpanzees, right whales and brown bears. Studies performed in captivity have been carried out
on pandas and pigs.

Finally, deep learning is being applied to automatically detect and classify the behavior of animals from raw data, a crucial step towards overcoming time-consuming and error-prone manual labeling tasks. Largely based on CNNs, a number of different solutions have been developed to recognize and label behaviors from images as well as video and sound recordings. These behavior detection systems can discriminate between behaviors, with the possibility of concurrent behaviors and thus multi-labeling, or be specifically designed to detect binary events (e.g. distinguish whale vocalizations from noise, or rare social changes in otherwise stable insect colonies). In addition to behavior recognition, deep learning solutions are also being devised to predict behavioral measurements that would otherwise require specialized recording devices. For example, Browning et al. used artificial neural networks to predict the diving behavior of seabirds from GPS data alone without specialized time-depth records,
whereas Liu et al. used vertical movement sensors alone to predict locomotor energy expenditure of sharks.

### 2.4 Genomics, population genetics, and phylogenetics

A rapidly growing number of studies apply deep learning to study genomes. Deep learning is used in DNA sequencing for translating the raw signal of long-read Oxford Nanopore sequencers into nucleotide calls, outperforming other basecallers. Another example of successful application is variant calling, or identification of small nucleotide polymorphisms and indels in diploid or polyploid genomes. DeepVariant is a tool that converts text file representations of multiple sequences aligned to a reference (read pileups) to images and uses a CNN to predict alternative alleles. Another tool predicts gene copy number variations from high-throughput sequencing reads. Deep learning has been particularly successful in functional and regulatory genomics and has been used for predicting sequence specificity of nucleic acidbinding proteins, methylation status, identification of transcription start sites, predicting expression patterns from genotypes, classification of transposable elements, and more. These applications are not strictly within the purview of ecology and evolution and have been comprehensively reviewed elsewhere.

Deep learning is a part of a growing trend to apply machine learning to the study of evolution of populations and species. One of the early studies applying neural networks to population genetic data showed them capable of estimating population-scale mutation rates, population sizes and their changes through time, recombination rates,
and detecting introgressed loci and positive selection on simulated data. That study
demonstrated that CNNs are capable of estimating population genetic parameters in scenarios for which likelihood-based methods have yet to be developed, such as accurately inferring recombination rates from read coverage data in autotetraploid genomes. The impressive performance of deep learning for population genetics encouraged recent development of user-friendly tools for inference from empirical data, including selective sweep classification, quantifying selection strength, jointly inferring selection and population size change, and inferring recombination landscapes. Other studies relied on custom approaches to identifying deleterious variants in sorghum and positive selection in SARS-CoV-2. An emerging approach involves combining deep learning with approximate Bayesian computation (ABC). It has been applied to inferring population size change through time, identifying hybridization from pairwise nucleotide divergences, and choosing best-fitting demographic scenarios based on site frequency spectra or SNP data. Most of the above approaches use CNNs, which in their standard formulation are sensitive to permutations. This means that the ordering of chromosomes in the input, for example, is significant for training and prediction. Flagel et al. dealt with this by sorting chromosomes by similarity but network architectures insensitive to input ordering are
also being developed.

Deep learning has also been used for inference and visualization of population structure. Here neural networks are used for dimensionality reduction, similar to
principal component analysis, rather than for solving a classification or regression problem. To achieve this, the authors used variational autoencoders (VAEs), a pair of neural networks that learn efficient representations of data in an unsupervised manner. In this method the encoder network compresses input data into latent variables, while the decoder network attempts reconstructing the original data from those variables. The loss function in this case is a combined measure of how good the reconstruction is and desirable properties of latent variables. The goal was to visualize population structure in a two-dimensional space and so the data were compressed into two variables representing coordinates.

As the importance of the spatial component is becoming increasingly highlighted in population genetics, deep learning is also beginning to be used for predicting sample origins based on genetic variation and local-ancestry inference, which aims to identify populations from which a genetic locus descended. This application involves using generative adversarial networks (GANs) to create artificial human genomic sequences of known ancestry.

