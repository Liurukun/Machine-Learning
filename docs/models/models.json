[
  {
    "path": "models/2023-10-15-advanced-machine-learning/",
    "title": "Lesson 2: Advanced Machine Learning",
    "description": "Unlike classic mechine learning in which features are extracted manually, deep learning (DL) can automatically extract features from data. When the amount of data is increased, DL is an effective algorithm, and gives better performance than classic machine larning. This section will guide you how to build DL with your own data.",
    "author": [],
    "date": "2023-10-31",
    "categories": [],
    "contents": "\n\nContents\n1. Building and running DL with R\n1.1 R packages for neural networks\n1.2 Setting R env for running keras\n\n2. Artificial Neural Network\n2.1 The architechture of ANN\n2.2 Constructing ANN networks\n2.3 Compiling neutral networks\n2.4 Improving network models\n2.5 Examples of ANN\n\n3. Convolutional Neural Network\n3.1 The architecture of CNN\n3.2 Basic conception of CNN\n3.3 An example of defining and training CNN Model\n\n4. The application of DL to ecology\n\nDL is a subfield of machine learning that works with algorithms inspired by the structure and functions of the human brain called “neural networks”. It has 3 types of neural networks: Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), and has a large number of potential applications in ecology, such as image recognition, analysis of acoustic signals, or any other tasks.\n1. Building and running DL with R\n1.1 R packages for neural networks\nThere are some packages that can fit basic neural networks. For example, the nnet package can fit feed-forward neural networks with one hidden layer. The neuralnet package fits neural networks with multiple hidden layers using back-propagation. The RSNNS package makes many model components for training a wide variety of models. The deepnet package provides a number of tools for deep learning, as well as allows for different activation functions and the use of dropout for regularization.\nThe h2o (https://www.h2o.ai/) is an excellent, general machine learning framework written in Java, and has an API that allows you to use it from R. However, most deep learning practitioners prefer other deep learning libraries, such as TensorFlow, CNTK, and MXNet. Keras is actually a frontend abstraction for other deep learning libraries, and can use TensorFlow in the background.\n1.2 Setting R env for running keras\nKeras is a high-level, open source, deep learning framework that emphasizes iterative and fast development. Keras models can be deployed on practically any environment, such as a web server, iOS, Android, a browser, or the Raspberry Pi. Now using Tensorflow’s Keras is recommended over the standalone keras package.\nTo learn more about using Keras in R, go to https://keras.rstudio.com; this link will also has more examples of R and Keras, as well as a handy Keras cheat sheet that gives a thorough reference to all of the functionality of the Keras package. Please pay much attention to the link of the different languages, and understand it for operating on them.\n\nFor running keras in Rstudio, you should make sure which python that you can use, and create a virtual environment, such as python3.8-venv.\n\n\n# library(reticulate)\n# py_config() # for environment information\n# install_python # for linux OS \n# install_miniconda() # for win OS\n\n\nYou can install the python packages of tensorflow and keras using py_install() function from reticulate or open the terminal within Rstudio to install them using pip. After that, you should install the R packages of keras and tensorflow.\n\n\n\n2. Artificial Neural Network\n2.1 The architechture of ANN\nANN is a forward propagation network that is consists of artificial neurons together and grouped in different levels known as layers. These are the 3 layers applied in structured artificial neural networks: Input Layer, Hidden Layer and Output Layer. This network is like this:\n\nThe architechture of ANN is different from that ofRNN, whichise used in natural language and speech recognition applications by processing sequential data. RNN can process different types of data at various moments in time. Its Architecture is like this.\n\n2.2 Constructing ANN networks\nA Keras model is the Sequential or functional class. Here we focus on the sequential model and pay attention to: 1) creating a fully-connected neural network architecture, 2) applying neural nets to regression and classification, 3) training neural nets with stochastic gradient descent, and 4) improving performance with dropout, batch normalization, and other techniques. For more details, see the site\nNeural networks are composed of neurons, each of which individually performs only a simple computation (see the box1).\n\n\n\nBox 1: Basic knowledge of neural networks\n\n\nSingle Input\nA larger network starts with a single neuron model as a baseline. Single neuron models are linear models. For example, training a model with ‘sugars’ as input and ‘calories’ as output in the dataset of cereals like this.\n\n\n\nThe formula for this neuron would be \\(y=wx+b\\)\nMultiple Inputs\nIf we wanted to expand our model to include things like fiber or protein content, we can just add more input connections to the neuron, and multiply each input to its connection weight and then add them all together.\n\n\n\nThe formula for this neuron would be \\(y=w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+b\\).\n\n\n\n\n\nWe can define the sequential model for allowing a neural network to “learns” by modifying the weight of each feature.\n\n\n# Create a network with one linear unit\n#model = keras.Sequential([\n#    layers.Dense(units=1, input_shape=[3])\n#])\n\n\nThe first argument, units, defining how many outputs we want. In this case just predicting ‘calories’, so units=1. The second argument, input_shape, telling Keras the dimensions of the inputs. Setting input_shape=[3] ensures the model to accept three features as input (‘sugars’, ‘fiber’, and ‘protein’). This model is now ready to be fit to training data!\nWe can combine and modify these single units to model more complex relationships that deep neural nets work for. Let’s see the box2 to understand how we stack layers to get complex data transformations. We have some nonlinearity with activation functions (see the box2).\n\n\n\nBox 2: Constructing neural networks\n\n\nLayers\nWe collect together linear units, and then get a dense layer through a common set of inputs.\n\n\n\nEach layer in a neural network performs relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways.\nActivation Function\nWithout activation functions, neural networks can only learn linear relationships (lines and planes). So we need are activation functions to fit curves. The common activation function is the rectifier function max(0,x).\n\n\n\nWhen we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. Applying a ReLU activation to a linear unit means the output becomes \\(max(0, w * x + b)\\). We stack layers inside a Sequential model, and add an activation function after the hidden layers, the network has the ability to learn more complex (non-linear) relationships in the data.\n\n\n\nNotice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n\n\n\n\n\n\nThe Sequential model we’ve been using will connect together a list of layers in order from first layer (input) to last layer (output). This creates a model in the above figure.\n\n\n#model = keras.Sequential([\n    # the hidden ReLU layers\n#    layers.Dense(units=4, activation='relu', input_shape=[2]),\n#    layers.Dense(units=3, activation='relu'),\n#    # the linear output layer \n##])\n\n\nAs for the dataset of cereals, we’ve chosen a three-layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data.\n\n\n#model = keras.Sequential([\n#    layers.Dense(512, activation='relu', input_shape=[11]),\n#    layers.Dense(512, activation='relu'),\n#    layers.Dense(512, activation='relu'),\n#    layers.Dense(1),\n#])\n\n\n2.3 Compiling neutral networks\nTraining a network model is to adjust its weights so that it can transform the features (inputs) into the target (output). If successfully train a network, its weights must represent some relationships between those features and that target (see the box3).\n\n\n\nBox 3: Compiling neural networks\n\n\nThe Loss Function\nThe loss function measures the disparity between the the target’s true value and the value the model predicts. A common loss function for regression is the mean absolute error (MAE), i.e., abs(y_true - y_pred). Besides MAE, other loss functions for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras). For classification, the common function is to measure the accuracy.\nOptimizer, minibatch and epoch\nAll of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent (SGD). They are iterative algorithms that train a network in steps like this:\nSample some training data and run it through the network to make predictions.\nMeasure the loss between the predictions and the true values.\nFinally, adjust the weights in a direction that makes the loss smaller.\nAdam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning. Adam is a great general-purpose optimizer. RMSpro is a good one for regression problems.\nEach iteration’s sample of training data is called a minibatch (often “batch”), while a complete round of the training data is called an epoch. The number of epochs you train is how many times the network will see each training example.\nLearning Rate and Batch Size\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds.\n\n\n\n\n\n\nAfter understanding these things, we should compile the squential model with the optimizer and loss function.\n\n\n#model.compile(\n#    optimizer='adam',\n#    loss='mae',\n#)\n\n\nNow we’re ready to start the training! We’ve told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\n\n\n#history = model.fit(\n#    X_train, y_train,\n#    validation_data=(X_valid, y_valid),\n#    batch_size=256,\n#    epochs=10,\n#)\n\n\n2.4 Improving network models\nCapacity: A model’s capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this is largely determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting data, you should try increasing its capacity.\nYou can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n\n\n#model = keras.Sequential([\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(1),\n#])\n\n#wider = keras.Sequential([\n#    layers.Dense(32, activation='relu'),\n#    layers.Dense(1),\n#])\n\n#deeper = keras.Sequential([\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(1),\n#])\n\n\nEarly Stopping: Keras keeps a history of the training and validation loss over the epochs. We’ll examine the learning curves for evidence of underfitting and overfitting for correcting it (see the box4).\n\n\n\nBox 4: Improving neural networks\n\n\nLearning Curves\nWhen we train a model we’ve been plotting the loss on the training set epoch by epoch. To this we’ll add a plot the validation data too. These plots we call the learning curves.\n\n\nWhen a model learns signal both curves go down, but when it learns noise a gap is created in the curves. You should create models that minimize the size of the gap.\nEarly Stopping\nWhen a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn’t decreasing anymore. Interrupting the training this way is called early stopping.\n\n\nOnce we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won’t continue to learn noise and overfit the data.\n\n\n\n\n\n\nIn Keras, we include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch.\n\n\n#early_stopping = callbacks.EarlyStopping(\n#    min_delta=0.001, # minimium amount of change to count as an improvement\n#    patience=20, # how many epochs to wait before stopping\n#    restore_best_weights=True,\n#)\n\n\nSpecial layers: There are some special layers that do not contain any neurons, but that they are added prevent overfitting and stabilize training. See the box5\n\n\n\nBox 5: Adding special layers\n\n\nDropout\nWe randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust. If you’re familiar with random forests as an ensemble of decision trees, it’s the same idea.\n\n\nBatch Normalization\nIt’s generally a good idea to put all of your data on a common scale. If it’s good to normalize the data before it goes into the network, maybe normalizing inside the network would be better! This special kind of layer can do this. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then putting the data on a new scale with two trainable rescaling parameters.\nBatch normalization is another special layer, which can help correct training that is slow or unstable.\n\n\n\n\n\n\nIn Keras, the dropout rate argument rate defines what percentage of the input units to shut off. Put the Dropout layer just before the layer you want the dropout applied to:\n\n\n#keras.Sequential([\n#    # ...\n#    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n#    layers.Dense(16),\n#    # ...\n#])\n\n\nAs for batch normalization layers, you can put it after a layer…\n\n\n#layers.Dense(16, activation='relu'),\n#layers.BatchNormalization(),\n\n\n… or between a layer and its activation function:\n\n\n#layers.Dense(16),\n#layers.BatchNormalization(),\n#layers.Activation('relu'),\n\n\nAnd if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn’s StandardScaler.\n2.5 Examples of ANN\nYou are suggested to visit the site or another site\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Convolutional Neural Network\n3.1 The architecture of CNN\nCNN is designed for image recognition. They can capture the spatial characteristics of an image. They work in a way like this.\n\nThe architectures contain three kinds of layers: convolutional layer, pooling layer, and fully-connected layer. For details about how each layer works, see here, as well as others.\n3.2 Basic conception of CNN\nThere is much information about CNN conception. You can watch this video.\n3.2.1 Convolution on an image\nLet’s take a look this imagine. It has 8x8 pixels, the values of which range from 0 to 255, representing black and white, respectively. For convolution on the imagine, see the box1.\n\n\n\nBox 6: Convoluting on an imagine\n\n\nconvolution operation: Convolution is a fundamental operation in image processing because it is a highly effective way to extract features and filter noise from images. The convolution operation involves sliding the filter over the image, computing the dot product between the filter and the corresponding image pixels at each position, and assigning the result to the central pixel of the filter.\n\n\n\nYou can use multiple filters extract pixel values for getting multiple feature maps, other than only one.\n\n\n\nIf convolution on a color image (i.e., RGB, three channels), you should select a filter with same channels to combined their values for a feature map like this.\n\n\n\nFor the convolution operation, you should notice the padding and stride. Padding and stride are two techniques used to improve convolutions operations and make the more efficient.\npadding: CNNs commonly use convolution filters with odd height and width values, such as 1, 3, 5, or 7 to avoid undesirable shrinkage of original imagines.\n\n\n\nstride: The amount of movement between applications of the filter to the input image is referred to as the stride. The stride of the filter on the input image can be used to downsample the size of the output feature map.\n\n\n\n\n\n\n\n\n\n3.2.2 Pooling operation\nAfter extracting features with filters, you can reduce data dimension and emphasize features by pooling layers (see the box7).\n\n\n\nBox 7: Pooling on a feature map\n\n\npooling operation: The pooling operation involves sliding a two-dimensional filter over each feature map and summarizing the features lying within the region covered by the filter. This makes the model more robust to variations in the position of the features in the input image.\n\n\n\n\n\n\n\n\n\n3.2.3 Flattening and activation functions\nAfter convolution and pooling steps, we are literally going to flatten our pooled feature map into a column like in the image below to feed ANN later on.\n\n\n\nBox 8: Flattening for a columon\n\n\nBasically, flattening just takes the numbers row by row, and put them into this one long column.\n\n\n\nActivation function: First note that the hidden and output layers usually have activation functions such as sigmoid, tanh, ReLU, identity, etc. Activation functions in hidden layers are usually nonlinear, e.g. ReLU (rectified linear unit), which is a piecewise linear function that introduces the most simple nonlinearity. ReLU Those in an output layer used are linear function g(z)=z, such as softmax.\n\n\n\n\n\n\n\n\n\n3.3 An example of defining and training CNN Model\n3.3.1 Designing CNN model\nNow there have some pictures of animals with dogs, cats and loans. There are many architectures for image classification, one of the most popular being convolutional neural networks (CNNs). CNNs are especially effective at image classification because they are able to automatically learn the spatial hierarchies of features, such as edges, textures, and shapes, which are important for recognizing objects in images.\nWe define the CNN architecture using the Keras library. The model will consist of several convolutional layers followed by max pooling layers, and a fully connected layer with a softmax activation function (see the box9).\n\n\n\nBox 9: Designing a CNN model\n\n\nHere is a CNN architecture for the classification of animal picture.\n\n\n\n\n\n\n\n\n\n\n3.3.2 Defining and training CNN model\nFor the code, please visit the website\n4. The application of DL to ecology\nPrediction of farmland fertility: The networks receive images from satellites like LSAT and can use this information to classify lands based on their level of cultivation. Consequently, this data can be used for making predictions about the fertility level of the grounds or developing a strategy for the optimal use of farmland.\nTopological analysis: A key aim of ecosystem analysis is to determine how stable an ecosystem is, and how it might react to environmental change. Therefore, a large amount of research has been concerned with determining the stability of ecosystems using theoretical techniques. These ideas were introduced by Elton, who described a community matrix M of size n × n for which the (i, j)-th entry represents the impact that a species h has on another species j around an equilibrium point of some unobserved dynamical system. This allowed stability analysis techniques from dynamical systems literature to be used, where a system is considered stable to small perturbations if the eigenvalues λ of M all have negative real parts. The original study was concerned with random community matrices, but the same ideas have since been refined and applied to realistic community matrices. This area of research has been used to contribute to the ongoing debate about the relationship between biodiversity and ecosystem stability.\nSpecies importance metrics: It is often of interest to ecologists which species are the most influential within their ecosystem. These species are known as keystone species. This has consequences in conservation, since the loss of a keystone species could lead to the collapse of an entire ecosystem. Traditionally, keystone species were identified from their natural history, but it is often difficult to experimentally verify this. This has led to the introduction of graph-theoretic species importance metrics which aim to discover keystone species by considering the topology of the network.\nNetwork structure prediction: Machine learning algorithms are able to discover patterns in data, and use those patterns to make predictions about previously unseen data samples. This had led to widespread use of ML in many different domains to automate laborious tasks which are expensive in terms of both time and money. Ecological networks are generally constructed in such ways, and therefore it is highly desirable to develop machine learning algorithms which automate the process. In addition,mMissing link prediction - as the name suggests - is the problem of predicting which links are missing from a network. There is a wealth of literature in network analysis concerning this problem and its applications to a range of complex networks. Recently, attention has turned to the problem of link prediction in the context of ecological networks. This has immediately useful practical applications in ecology. The randomness present in an ecosystem makes it unlikely that all actual interactions are observed when collecting data to construct ecological networks. Therefore, the use of link prediction algorithms could help ecologists to discover actual unobserved interactions when used in conjunction with traditional ecological network construction methods.\n\n\n\n",
    "preview": "models/2023-10-15-advanced-machine-learning/multi_layer.png",
    "last_modified": "2023-10-31T10:25:54+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-17-one-shot-learning-with-r/",
    "title": "Lesson 6: One Shot Learning with R",
    "description": "One-shot learning is a machine learning algorithm using a limited amount of training set data to compare the similarities and differences between two images. This section will introduce you touUse R keras for building self define generator, building self define layer, building self define backend function.",
    "author": [],
    "date": "2023-10-18",
    "categories": [],
    "contents": "\n\nContents\nExample 1: Recognization of mnist number\n\nExample 1: Recognization of mnist number\nStep 1 : Data Load\nRead Data and Split into train、validate and test\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-common-machine-learning/",
    "title": "Lesson 1: Common machine learning",
    "description": "Conservation science depends on an accurate understanding of what's happening in a given ecosystem. How many species live there? What is the makeup of the population? How is that changing over time? Species Distribution Modeling seeks to predict the spatial (and sometimes temporal) patterns of species occurrence, i.e. where a species is likely to be found. The last few years have seen a surge of interest in applying powerful machine learning tools to challenging problems in ecology. Our goal in this work is to introduce ecologically useful ML-based algorithms.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. From statistics to machine learning\n2. Machine learning and its main types\n3. Training models with R packages\n3.1 Data collection and importing\n3.2 Exploratory Data Analysis (EDA)\n3.3 Data Preprocessing\n3.4 Model training and Evaluation\n\n4. other resoures\n\n1. From statistics to machine learning\nLinear regression analysis estimates the value of one variable relative to the value of another. Linear regression is frequently used in statistical data analysis, i.e., according to least squares, you can calculate its coefficient and intercept. For more details, please check the book. The model performance is examined by \\(R^2\\), and the significance for \\(R^2\\).\n.\nLinear regression is also achieved via a machine learning algorithm. That is, gradient descent (GD). The operation of GD works by starting with random values for each coefficient. The sum of the squared errors are calculated for each pair of input and output values. A learning rate is used as a scale factor and the coefficients are updated in the direction towards minimizing the sum square errors. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.\n2. Machine learning and its main types\nThe two main paradigms of ML are supervised and unsupervised learning. Supervised learning is a subfield of ML concerned with finding a function f such that \\(\\hat{y} = f(x)\\) where x is an input sample vector, y is an output sample vector, and \\(\\hat{y}\\) is a predictor of y. Common supervised learning tasks include classification and regression. Unsupervised learning aims to find a function g which transforms an input\nsample x to a representation z in order to reveal underlying information about the input sample. A common unsupervised learning task is clustering.\nIn machine learning algorithms, a parameterisable function \\(f_{θ}\\) is often defined. The parameters θ of the function can then be learned through an iterative process of updating the parameters and evaluating the performance of the function. This process is known as optimisation or training; these training algorithms often rely on a\nfunction \\(L(y, \\hat{y})\\) which quantifies how incorrect a prediction is compared to the target vector. This is usually known as the cost, error, or loss function, and is chosen depending on the task in hand.\nDuring optimisation, parameters are adjusted according to training data. In the case of supervised learning, training data comprises pairs of input and output vectors, each taking the form (x, y). The algorithm will be shown each data sample multiple times. The number of times an algorithm “sees” the entire set of training data is known as an epoch, and is used as a measure of how much an algorithm has been trained. In some circumstances, an algorithm can perform well on the training data but does not perform well on new data. This is known as overfitting, and occurs when the algorithm has learned to predict the target output of each sample in the training data by random noise in the input features, rather than by the important underlying variables.\nOverfitting can be detected by splitting the training data into three sets: training, validation, and test. Under this split, the algorithm is trained on the training set, and after each epoch is evaluated on the validation set. When the performance on the validation set does not increase, the algorithm has stopped learning useful properties and has begun to overfit. The algorithm can then be stopped -known as early-stopping - and evaluated on the unseen test set to give a true indication of the algorithm’s performance. The general configuration of the function f is usually governed by hyperparameters, which - unlike parameters - are fixed and are not adjusted during training.\nML algorithms are attractive options for solving some problems, because the learned functions \\(f_{θ}\\) are derived directly from the training data without intervention. This makes ML algorithms particularly useful on complex problems for which it is difficult or near-impossible to manually define suitable functions. However, the usefulness of ML is not limited to predictive tasks; after training the learned function can also be interpreted to yield useful information about the data.\n3. Training models with R packages\nBelow, we’ll examine fundamental machine learning ideas, methods, and a step-by-step procedure of machine learning model developments by utilizing Caret package. Please check this website for details. In this section, we need the libraries, including:\nggplot2: for interactive graphs and visualization.\nggpubr: for making plot beautiful along with that of ggplot2.\nreshape: for melting dataset.\ncaret: providing many machine learning algorithms.\n\n\n\nWe will walk through each step of implementing Caret package in this part. The general steps to be followed in any Machine learning project are:\n3.1 Data collection and importing\nNext, we will import our data to a R environment.\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n3.2 Exploratory Data Analysis (EDA)\nUnderstanding and assessing the data you have for your project is one of the important steps in the modeling preparation process. This is accomplished through the use of data exploration, visualization, and statistical data summarization with a measure of central tendencies. You will gain an understanding of your data during this phase, and you will take a broad view of it to get ready for the modeling step.\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nVisualizing the outliers by using boxplot. As we use ggplot2 we will take numerical variables by subsetting the entire of it. Using of reshape package we melt the data and plot it to check for the presence of any outliers.\n\n\n\nLet’s now use a histogram plot to visualize the distribution of our data’s continuous variables.\n\n\n\nNext, we will move to the Data Preparation phase of our machine learning process. Before that, lets split our dataset into train, test and validation partition.\n\n\n\n3.3 Data Preprocessing\nThe quality of our good predictions from the model depends on the quality of the data itself, data preprocesing is one of the most important steps in machine learning. We can see from the box plot that there are outliers in our data, and the histogram also shows how skewed the data is on the right and left sides. We shall thus eliminate those outliers from our data.\n\n\n\nAfter obtaining the quantile value, we will additionally compute the interquartile range in order to determine the upper and lower bound cutoff values. Then, we eliminate the outliers.\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n48           4.6         3.2          1.4         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\nBy using a boxplot, we can additionally see the outliers that were eliminated from the data.\n\n\n\n3.4 Model training and Evaluation\nIt’s time to use the clean data to create a model. We don’t have a specific algorithm in mind, Let’s compare LDA and SVM for practical purposes and choose the best one. For accuracy and prediction across all samples, we will employ 10-fold cross validation.\n\n\n\nLet’s start training model with Linear Discriminant Analysis.\n\nLinear Discriminant Analysis \n\n120 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 108, 108, 108, 108, 108, 108, ... \nResampling results:\n\n  Accuracy  Kappa \n  0.975     0.9625\n\nWe can also use SVM model for the training.\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n120 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 108, 108, 108, 108, 108, 108, ... \nResampling results across tuning parameters:\n\n  C     Accuracy   Kappa\n  0.25  0.9333333  0.900\n  0.50  0.9500000  0.925\n  1.00  0.9333333  0.900\n\nTuning parameter 'sigma' was held constant at a value of 0.7381148\nAccuracy was used to select the optimal model using the\n largest value.\nThe final values used for the model were sigma = 0.7381148 and C\n = 0.5.\n\nThe results show that both algorithms functioned admirably with only minor variations. Although the model can be tuned to improve its accuracy accurate, for the purposes of this lesson, let’s stick with LDA and generate predictions using test data.\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0         10         0\n  virginica       0          0        10\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8843, 1)\n    No Information Rate : 0.3333     \n    P-Value [Acc > NIR] : 4.857e-15  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           1.0000\nSpecificity                 1.0000            1.0000           1.0000\nPos Pred Value              1.0000            1.0000           1.0000\nNeg Pred Value              1.0000            1.0000           1.0000\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.3333\nDetection Prevalence        0.3333            0.3333           0.3333\nBalanced Accuracy           1.0000            1.0000           1.0000\n\nAccording to the summary of our model above, We see that the prediction performance is poor; this may be because we neglected to consider the LDA algorithm’s premise that the predictor variables should have the same variance, which is accomplished by scaling those features. We won’t deviate from the topic of this lesson because we are interested in developing machine learning utilizing the Caret module in R.\n4. other resoures\nHere is a concise guide to machine learning techniques for ecological data. This practical guide to machine learning includes explaining and exploring different machine learning techniques, from CARTs to GBMs, using R.\n\n\n\n",
    "preview": "models/2023-10-15-common-machine-learning/distill-preview.png",
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-deep-learning-as-data-driven-methods/",
    "title": "Lesson 3: Deep Learning as Data-driven Methods",
    "description": "The immediate aim of this section is to develop a data-driven machine learning algorithm to predict which interactions are missing from ecological networks, and to explore ways in which ecological insight can be extracted from the algorithm. In particular, we introduce key concepts and terminology of data-driven models.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. What are neural networks and how do they learn?\n2. Appplications of deep learning to ecology and evolution\n2.1 Automated species identification\n2.2 Environmental monitoring and modeling\n2.3 Behavioral studies\n2.4 Genomics, population genetics, and phylogenetics\n\n\nEcology and evolutionary biology investigate complex patterns and processes. A mathematical toolkit has been necessary to describe and explain fundamental components of organic evolution and ecological interactions. This wealth of data is driving the development of analytic tools that can provide new understanding, greater efficiency, and ease of use. Likelihood-based mechanistic approaches designed to consider many variables can be so computationally expensive that they can no longer be applied to data routinely generated in modern studies. A promising alternative is likelihood-free inference, one example of which is machine learning. The goal of machine learning is to find a model that performs well at making predictions from the data. This contrasts with likelihood-based methods, which assume the model generating the data is known. More recently machine learning has seen a dramatic surge in popularity\nwith a slew of new algorithms and applications.\nOne of the approaches rapidly gaining popularity is deep learning. Deep learning relies on multilayered, connected processing units (artificial neural networks or ANNs). The successes of deep learning were possible because of a major advantage of deep learning over classical machine learning approaches. Classical machine learning requires that important data features are identified using expert domain knowledge. Neural networks can automatically discover the most important data features and patterns relevant for the task. Researchers are now beginning to apply deep learning to problems across ecology and evolutionary biology, from community science projects and environmental monitoring through sequencing equipment output processing, to population genetics\nand phylogenetic inference.\n1. What are neural networks and how do they learn?\nHere describe what artificial neural networks are and how they are used as inference tools (Box 1).\n\n\n\nBox 1: Common neural network architectures\n\n\nArtificial Neural Networks(ANNs) is a group of multiple perceptrons/ neurons at each layer. Neurons can be connected to other neurons in different layers but not within the same layer. In the simplest form, a neural network has one input layer, at least one intermediate or hidden layer, and an output layer. The trainable network parameters consist of biases and weights. Each node has a bias value (b), which determine how easy it is for it to “fire”. Each connection has a weight value (W) which represents connection strength. Each node also has an activation function (f), for example sigmoid\nfunction. Given some input (x), the so-called feedforward output (y) of a node is determined by a simple equation: \\(y = f(W × x + b)\\).\nANN is a Feed-Forward Neural network because inputs are processed only in the forward direction. Recurrent neural networks is to add loops to information flow. Information flows from the input to the output of the network but it can flow back from the output to the input of the hidden layer through recurrent weights.\n\n\n\nHowever, simple RNNs such as the one shown in the figure are difficult to train because weights in these networks can quickly diverge during training. More advanced RNN, such as Long Short-Term Memory networks (LSTMs) or Gated Recurrent Units (GRUs), address this problem and are commonly used with time series data. In evolutionary biology, deep learning solutions including GRU have been used to predict recombination landscapes. In the field of ecology. reserchers are trying to use deep learning techonologies for the analysis of remote sensing data. The convolutional neural networks (CNNs) can excel at capturing complex, hierarchical patterns and are the architecture used in most identification and classification problems.\n\n\n\n\nIn mathematical sense, neural networks are simply a function mapping input onto a desired output. This general design is simple, but it makes neural networks\nextraordinarily powerful: a network with information flowing from input to output layer with at least one intermediate layer (i.e. feedforward network) can approximate any continuous function, regardless of its complexity. These approximations can describe pixels of an image, for example, and networks with multiple intermediate layers (deep neural networks) can also learn relationships between them as high-level concepts such as lines, geometric shapes, and even whole scenes.\nANNs learn continuous distributions but their output can represent probabilities of distinct data classes, as well as continuous values. Such networks can thus be used to construct classifiers, which are models distinguishing among discrete categories, as well as regression models, which infer continuous values. But feedforward operations alone do not allow the network to learn or generalize to new data, which is the essence of most ANN applications.\nIn order for an ANN to be a predictive tool, one needs to assess how good predictions are and be able to adjust ANN parameters to improve performance. A measure of how far off the output of the network is is called a loss function. One example of a loss\nfunction is the sum of squares error (SSE), which is simply the sum of differences between each predicted value (y) and the true value (\\(\\overline{y}\\)) squared for absolute value:\n\\[SEE= \\sum_{i = 1}^{n}(y_i - \\overline{y}_i)^2\\]\nThe network also needs a mechanism for finding the set of parameters that minimize the loss function. Once the loss (error) is measured at the output, it has to be traced back across the network to measure how parameters contributed to it. This process is called backpropagation and it uses chain rule calculus to find the derivative (slope) of the loss function with respect to the network’s trainable parameters. The process of increasing or decreasing parameters such that they minimize the derivative of the loss function is called gradient descent. This process is iterative, occurring every time a batch of training data is processed, and collectively referred to as the training loop. When devised correctly, it results in improvement of inference accuracy with each pass of the loop.\nThe fact that ANNs are universal approximators for continuous functions that can be trained makes them powerful predictive tools. This learning scheme is most easily illustrated with an example of supervised learning, where the network is trained on a data set, e.g. images of expert-identified species or methylated vs. unmethylated DNA sequences. Supervised training usually involves splitting data into three subsets:\ntraining, validation, and testing. The validation set is not directly used in training but prediction on it is performed at the end of each training cycle to assess how well the network generalizes outside of the training set. Test set is held back for the final estimate of accuracy.\n2. Appplications of deep learning to ecology and evolution\nIn the sections that follow, we review how deep learning has been applied in ecology and evolution, including species identification and monitoring, ecological and behavioral studies, and population genetics and phylogenetics. We use these examples to showcase the variety of deep learning techniques extending its usage beyond the general picture described above.\n2.1 Automated species identification\nDeep learning enabled breakthroughs in automated image classification, largely possible thanks to CNNs. Image recognition has obvious applications in biology and was\nadopted early for problems of species identification and wildlife monitoring. It is not surprising then that identification or classification of individuals or species from image, video, and sound data is the most common use of deep learning in the field. These efforts already span many taxa, from bacteria, through protozoans, plants to insects and vertebrates, both extant and fossil and at scales ranging from local to global. Intensifying efforts to digitize natural history collections provide troves of\nimage data that can be used for this purpose.\nCamera trap systems and deep learning classifiers are now commonly used for vertebrate wildlife monitoring and systems automating environmental monitoring of aquatic macroinvertebrates are also being developed. Many publications present systems or deep learning models for detecting and identifying pests or crop diseases in agroecosystems or stored agricultural\ncommodities. Despite economic importance and demonstrated potential for crop pest and disease monitoring, at present few non-proprietary systems or open source software\napplications exist. A notable exception is a mobile application system for identifying diseases of cassava plants, one of the most important tropical crops.\nDeep learning has also been applied to identification from audio recordings, including bird and bat sounds, and even wing beats of mosquitoes. Unsurprisingly, the technology has been applied most often to bird calls, where it has been used not only to identify species, but also monitor their abundance. The recently developed BirdNET is a deep neural network capable of identifying North American and European birds from vocalizations in complex soundscapes, available on a variety of platforms, including user-friendly smartphone apps. Most of these studies use audio converted to spectrograms, image representations of sound, to train CNNs as in visual recognition problems.\nGiven its utility for automated identification, deep learning is increasingly used in community science initiatives. Examples include a growing number of mobile phone applications such as plant-focused identification app Pl@ntNet, bird identification tool Merlin or the citizen naturalist portal iNaturalist, as well as a number of more local or taxon-specific guides. Many of these applications crowd-source training data collection and identification verification by users. They improve by periodically re-training their deep learning classifiers as more reliable data is collected.\nMany of these studies employ data handling approaches that increase performance of deep learning classifiers. Several use data augmentation, a technique that relies on altering training data with distortion. These modifications, applied to each data input in each training epoch, effectively increase training set size. Data augmentation is an important strategy for reducing overfitting and almost always results in increased classifier accuracy. By ensuring that the neural network never sees the same input twice, augmentation only partly addresses the fact that acquiring large, human-labeled datasets is a bottleneck for many applications. An alternative approach is to train an initial classifier in a supervised way, using a labeled training set, and then use this reasonably well-performing classifier for adding more images in an unsupervised manner, without human intervention.\nAnother technique ubiquitous in identification and classification tasks is transfer learning. Transfer learning is most commonly accomplished by first training on a different, usually larger and more general dataset than the one assembled for the problem on hand. The resulting network parameters can then be used as the starting point for fine-tuning on the focal dataset. In species recognition from images it is common to use networks pre-trained on large, public datasets of everyday objects such as ImageNet or COCO as illustrated by several of the studies cited above. Using pre-trained networks makes the network learn faster and often results in higher accuracy.\nIn addition to properly assigning a label to an image, termed image classification, a common computer vision problem is to localize objects. Object recognition is a term often used for the combination of the two: drawing a bounding box around an object and predicting its class. Because there may be many objects in an image, this is a more challenging problem. The many proposed solutions involve either extracting candidate regions from images prior to prediction or predicting classes directly on grids of image pixels. Examples are common in agriculture, where object detection has been used to identify and count pests.\nThe deep learning framework allows training several neural networks of the same or varying architectures on one dataset and averaging their predictions. Known as model ensembling, this technique reduces variance in predictions and can improve accuracy. Examples in species identification include Finnish fungi recognition and UK ladybird beetles.\nFinally, deep learning is not limited to considering image pixels alone but can also take advantage of contextual information such as locality or phenology. For example, output can be improved by filtering out\nnonsensical predictions given prior occurrence data. This approach, however, does not jointly consider the available data in a common framework. Neural networks can be trained on multiple data inputs simultaneously and consider them jointly in the final layers. One study used this approach for beetle identification from images and found improvement in accuracy with information about location, date, weather, habitat, and\nuser expertise.\n2.2 Environmental monitoring and modeling\nThe above mentioned approaches to automated identification of species or individuals are also being scaled to ecosystem scale and applied to diversity assessment, conservation, and resource management. Examples using techniques detailed\nin the previous section include detecting and estimating abundance of zooplankton\nand detecting and counting sea turtles and whales using drone and satellite imagery. Other uses combine digital imagery with LiDAR and other remote sensing or geospatial data for mapping of vegetation, forest carbon stock, and the footprint of fishing across the world’s oceans. Similar applications include integrated systems for real-time wildlife monitoring using data from camera traps and microphone and raise the prospect of surveillance of social media posts for illegal animal trade.\nIn addition to classification and mapping of static information, RNNs and similar approaches have been used with temporal ecological data. Examples include predicting\neutrophication, phytoplankton blooms, and benthic invertebrate community dynamics. As mentioned previously, combining inputs from different sources is natural for deep learning and Rammer and Seidl take advantage of this to predict and map future bark beetle outbreaks based on temporal information on climate, vegetation, and past outbreaks. Capinha et al. proposed a generalized approach to classification and prediction from ecological time series data leveraging automated choice of the best network architecture for the task at hand.\nFinally, neural networks are being used to develop more realistic models and simulations of real world patterns and phenomena. Benkendorf and Hawkins found that deep neural networks can be used to generate accurate species distribution models but\nalso noted that other machine learning approaches perform as well or better with limited training data. Strydom et al. designed a system to predict species interactions from co-occurrence data. A study using reinforcement learning investigated how learning to hunt or avoid predators by individual agents influenced predator-prey dynamics.\n2.3 Behavioral studies\nThe study of animal behavior, both in the field and controlled laboratory settings, is another research area of ecology and evolution that is poised to greatly benefit from adoption of deep learning. Recent technological advancements in sensing, monitoring, and automation allow behavioral ecologists to collect and analyze large amounts of data Long-standing challenges in identifying, quantifying, and analyzing animal behavior still limit the ability to fully automate processing of these data, however. Deep learning has the potential to address many of these challenges and it is increasingly being adopted in studies involving identification of individual animals, body posture and movement tracking, and classification of behaviors.\nIn the area of animal body posture, deep learning can provide non-invasive estimation of the position of animals’ body parts from video recordings. Several open-source toolkits have been developed for this purpose, ranging from species-specific solutions, to generic frameworks applicable to any species, some of which offer 3-dimensional and/or multiple animals tracking. In addition to pose estimation,\ndeep learning is also being adopted to enhance the performance of established computer vision methods used to track spatial position of animal detection or the identification of markers, as well as to automatically perform behavioral analysis of spatial trajectories.\nDeep learning can also allow for the identification, classification, and subsequent re-identification of individual animals from camera feeds or traps, both in the wild and in captivity. Usually based on the use of CNNs for image recognition, deep\nlearning can also be combined with other technologies to develop automated\ndata-processing pipelines to collect and label samples bird species. A popular application in this area is face recognition enabling mark-recapture studies for monitoring populations of individuals, their behavior, and social interactions. Examples in the wild include identification of elephants, chimpanzees, right whales and brown bears. Studies performed in captivity have been carried out\non pandas and pigs.\nFinally, deep learning is being applied to automatically detect and classify the behavior of animals from raw data, a crucial step towards overcoming time-consuming and error-prone manual labeling tasks. Largely based on CNNs, a number of different solutions have been developed to recognize and label behaviors from images as well as video and sound recordings. These behavior detection systems can discriminate between behaviors, with the possibility of concurrent behaviors and thus multi-labeling, or be specifically designed to detect binary events (e.g. distinguish whale vocalizations from noise, or rare social changes in otherwise stable insect colonies). In addition to behavior recognition, deep learning solutions are also being devised to predict behavioral measurements that would otherwise require specialized recording devices. For example, Browning et al. used artificial neural networks to predict the diving behavior of seabirds from GPS data alone without specialized time-depth records,\nwhereas Liu et al. used vertical movement sensors alone to predict locomotor energy expenditure of sharks.\n2.4 Genomics, population genetics, and phylogenetics\nA rapidly growing number of studies apply deep learning to study genomes. Deep learning is used in DNA sequencing for translating the raw signal of long-read Oxford Nanopore sequencers into nucleotide calls, outperforming other basecallers. Another example of successful application is variant calling, or identification of small nucleotide polymorphisms and indels in diploid or polyploid genomes. DeepVariant is a tool that converts text file representations of multiple sequences aligned to a reference (read pileups) to images and uses a CNN to predict alternative alleles. Another tool predicts gene copy number variations from high-throughput sequencing reads. Deep learning has been particularly successful in functional and regulatory genomics and has been used for predicting sequence specificity of nucleic acidbinding proteins, methylation status, identification of transcription start sites, predicting expression patterns from genotypes, classification of transposable elements, and more. These applications are not strictly within the purview of ecology and evolution and have been comprehensively reviewed elsewhere.\nDeep learning is a part of a growing trend to apply machine learning to the study of evolution of populations and species. One of the early studies applying neural networks to population genetic data showed them capable of estimating population-scale mutation rates, population sizes and their changes through time, recombination rates,\nand detecting introgressed loci and positive selection on simulated data. That study\ndemonstrated that CNNs are capable of estimating population genetic parameters in scenarios for which likelihood-based methods have yet to be developed, such as accurately inferring recombination rates from read coverage data in autotetraploid genomes. The impressive performance of deep learning for population genetics encouraged recent development of user-friendly tools for inference from empirical data, including selective sweep classification, quantifying selection strength, jointly inferring selection and population size change, and inferring recombination landscapes. Other studies relied on custom approaches to identifying deleterious variants in sorghum and positive selection in SARS-CoV-2. An emerging approach involves combining deep learning with approximate Bayesian computation (ABC). It has been applied to inferring population size change through time, identifying hybridization from pairwise nucleotide divergences, and choosing best-fitting demographic scenarios based on site frequency spectra or SNP data. Most of the above approaches use CNNs, which in their standard formulation are sensitive to permutations. This means that the ordering of chromosomes in the input, for example, is significant for training and prediction. Flagel et al. dealt with this by sorting chromosomes by similarity but network architectures insensitive to input ordering are\nalso being developed.\nDeep learning has also been used for inference and visualization of population structure. Here neural networks are used for dimensionality reduction, similar to\nprincipal component analysis, rather than for solving a classification or regression problem. To achieve this, the authors used variational autoencoders (VAEs), a pair of neural networks that learn efficient representations of data in an unsupervised manner. In this method the encoder network compresses input data into latent variables, while the decoder network attempts reconstructing the original data from those variables. The loss function in this case is a combined measure of how good the reconstruction is and desirable properties of latent variables. The goal was to visualize population structure in a two-dimensional space and so the data were compressed into two variables representing coordinates.\nAs the importance of the spatial component is becoming increasingly highlighted in population genetics, deep learning is also beginning to be used for predicting sample origins based on genetic variation and local-ancestry inference, which aims to identify populations from which a genetic locus descended. This application involves using generative adversarial networks (GANs) to create artificial human genomic sequences of known ancestry.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-remote-sensing-data-and-models/",
    "title": "Lesson 4: Remote sensing data and models",
    "description": "Remote sensing is the science of identifying, observing, collecting, and measuring objects without coming into direct contact with them. This can be accomplished through many devices that carry sensors and capture the characteristics of Earth remotely. Sensors on board satellites also record the electromagnetic energy that is reflected or emitted from objects on Earth. They are especially useful for natural resoures and a variety of socio-economic research and applications.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. Remote sensing data\n2. Data operations and tools\n2.1 Download data of aoi\n2.2 Merging, cropping and masking\n2.3 Extracting values and computing statistics\n2.4 Storing and exporting results\n\n3. A practical example\n\n1. Remote sensing data\nForest Cover Data\nThis section is adopted from the module. we will primarily work with the Vegetation Continuous Fields (VCF) data provided by the Land Processes Distributed Active Archive Center (LP DAAC), a component of NASA’s Earth Observing System Data and Information System (EOSDIS). The MOD44B Version 6 VCF is a yearly representation of surface vegetation from 2000 to 2020 at 250 m resolution. Each pixel stores a percentage of three ground cover components: percent tree cover, percent non-tree cover, and percent bare.\nThe ground cover percentages are estimates from a machine learning model based on the combination of the Moderate Resolution Imaging Spectroradiometer (MODIS) data and other high resolution data from NASA and Google Earth. The machine learning model incorporates the visible bandwidth as well as other bandwidth such as brightness temperature (from MODIS bands 20, 31, 32).\nThe VCF data utilize thermal signatures and other correlates to distinguish forest and non-forest plantation, which is an improvement compared to the Normalized Differenced Vegetation Index (NDVI). For this use case, VCF also improves on the Global Forest Cover (GFC) data set, another source used to study deforestation, which only provides binary data points. GFC records baseline forest cover in the year 2000 and includes a binary indicator for the year of deforestation for each 30m × 30m pixel. If over 90% of the forest cover was lost in a pixel by a given year, then the pixel is marked as deforested, while a pixel is marked as reforested if the forest cover went from 0 in 2000 to a positive value. The VCF records continuous changes in percent of ground cover components, which provides more details than the GFC data.\nNighttime lights\nThere is a strong correlation between nighttime lights and Gross State Product (GSP) or Gross Domestic Product (GDP) measures, at the national, state and regional levels or even at a more granular resolution. Thus, nighttime light observations can be used as a proxy for economic activity, especially over periods or regions where these data are not available or where the statistical systems are of low quality or when no recent population or economic censuses are available. Similarly, changes in nighttime light intensity can be used by economists as an additional measure of income growth when no other measures of income growth are available.\nProville et al. (2017) examined trends observed by DMSP-OLS in the period 1992-2013 and their correlation with a series of socio-economic indicators. They found the strongest correlations between nighttime lights, electricity consumption, CO2 emissions, and GDP, followed by population, CH4 emissions, N2O emissions, poverty and F-gas emissions.\n2. Data operations and tools\n2.1 Download data of aoi\nIn order to perform data manipulation, we need to attach packages. We are going to use the package luna to download data from MODIS and the packages terra, tidyverse, raster, and sf for data manipulation.\n\n\n\nWe follow thistutorial to get MODIS data with luna. For details of the terra package, please refer to the package manuscript and this tutorial. If you are not familiar with the tidyverse workflow, please refer to the R for Data Science.\nOnce the required packages have been attached, we can access VCF in R. We prefer using R for its ability to download large numbers of files and enable regular, automated updates.\nWe can first use luna to check the list of data products available from MODIS. Since luna can also access data from the LANDSAT and SENTINEL platforms, we add “MOD|MYD|^MCD” to narrow our scope to MODIS data. The printed results below list six products from MODIS.\n\n\n\nThe product name for VCF is MOD44B. We can use the function productInfo to launch the information page of VCF.\n\n\n\nWe can query MODIS and only download a subset of the data. We need to specify the start and end dates and our area of interest (AOI). The date format is “yyyy-mm-dd”. Suppose here we want to subset data from 2010 to 2012.\n\n\n\nIn order to subset your area of interest, you need to provide a “map” to getModis(). This can be obtained from online databases such as the global administrative area database (GADM). You can download map data directly from GADM or you can use R to obtain GADM map data. We will use R below, which requires first installing the package geodata.\n\n\n\nGeographic levels in GADM are defined as:\nlevel 0: National\nlevel 1: State/province/equivalent\nlevel 2: County/district/equivalent\nlevel 3/4: Smaller administrative levels\nFor our example, we are interested in India at the district level. We can download the map of India and its level 2 administrative areas with the following code:\n\n\n\nThe boundary data is downloaded to the path that you specified in the path argument. The downloaded data through gadm() will be in the PackedSpatVector class. If you want to convert it to another class (for example, the sf class, which is easier to work with in R), you can first read it using readRDS(), then convert to a SpatVector via vect() from the terra package, and finally convert it to a sf object.\n\n\n\nThe map we downloaded is at the district level (level 2). Assume our AOI is the state of Odisha. Each row of the data represents a county in Odisha, and the geospatial information for each county is stored in the last column: geometry. We can filter to obtain the boundaries for our AOI, which will return aoi in vector format, stored as a data frame in R.\n\n\n\n\n\n\nNow that we have our AOI as well as time frame, we can filter the MODIS VCF data on these values and see what is available.\n\n\n\nThe products we are going to download are tiled products. For details of tiled products, the tilling system, and the naming convention, please refer to the MODIS overview page. In essence, we will be downloading grids of maps that cover our AOI.\nTo actually download these files from the NASA server, you will need a username and password. Please register on NASA Earth Data if you haven’t done so.\nThe following code will download the files. Replace the path value with the location on your computer where you would like to store these files. Replace the username and password values with your NASA Earth Data credentials from above.\n\n\n\nThe data format from MODIS is HDF and may include sub-datasets. We can use terra to read these files and create raster files. For example,\n\n\n\nWe can find basic information such as the coordinate reference system, number of cells, and resolution from the above output. There are 7 layers in each of the VCF tiled files. We are interested in the percent tree coverage layer.\n\n\n\nA quick plot of the data can be done with the plotRBG() function.\n\n\n\n2.2 Merging, cropping and masking\nSince there are four hdf files in each year for our AOI, we can first merge the four SpatRaster files into one file per year. We’ll use 2010 as an example. We can filter to only include our layer of interest - percent of tree cover - from each hdf file, which can be done by subsetting the output using [[1]] (using 1 because percent tree cover is the first layer in each file).\n\n\n\nBefore we merge these SpatRster objects, it is often a good practice to check their origins and resolutions. merge requires origin and resolution to be the same across objects.\n\n\n\n\n\n\nWe see that origins of these files are slightly different, but all are close to (0, 0). We do not need to worry about these slight differences, as merge will handle them automatically.\n\n\n\nNote: cells with 200% represent water and rivers.\nWe are now ready to crop and mask the raster file to match our AOI. This tutorial explains the difference between cropping and masking.\nTo crop a raster file according to vector data boundaries (eg, our aoi object representing Odisha districts), we first align the coordinate reference systems of our raster file and vector file. Then, use crop(raster data, vector data). To mask, use mask(raster data, vector data). Note that for terra::mask(), the second argument needs to be SpatVector. terra does not support sf objects yet, so we use vect(aoi) to convert our sf object aoi to a SpatVector.\n\n\n\nTo plot our new raster file, we use:\n\n\n\n2.3 Extracting values and computing statistics\nAfter we have cropped and masked the raster file to our AOI, we can extract values for each county in the state of Odisha.\n\n\n\nThe values extracted by terra::extract are stored in a data frame. Note that the ID corresponds to the row number of your vector file (i.e. object aoi in our case). We can then compute statistics based on this data frame. Here we compute several statistics describing the percent of forest cover for each county. Note that cells with 200% represent water and river and should be excluded from calculation.\n\n\n\n2.4 Storing and exporting results\nWith terra you can easily write shape files and several formats of raster files. The main function for writing vector data is writeVector(), while for writing raster data we use writeRaster(). For details, you can refer to this page and the documentation of terra.\n3. A practical example\nWe will replicate some main results in the paper. To access the full replication data and code, check this github repo. We are going to replicate Table 3 in the paper.\nThe research question is whether newly constructed rural roads impact local deforestation. The authors explored this question using two empirical strategies: fuzzy RD and difference-in-difference. In the following sections, we implement the difference-in-difference method and replicate the regression results.\nIn order to run fixed effects models, we will need the fixest package. This tutorial is a good reference for introducing fixest functions.\nData for this exercise was processed and stored in pmgsy_trees_rural_panel.csv, which you can find the through the link to the CSV data in the github repo. Each row of the data frame presents a village in a specific year.\n\n\n\nThe paper estimated the following equation:\n\\[\nForest_{vdt} = β_{1}Award_{vdt} + β_{2}Complete_{vdt} + α_{v} + γ_{dt} + X_{v}⋅V_{t} + η_{vdt}\n\\]\nwhere \\(Forest_{vdt}\\) is forest cover of village \\(v\\) in district \\(d\\) in year \\(t\\). \\(Award_{vdt}\\) is a dummy variable which takes one during the period when the new road is awarded to the village but has not been built. \\(Complete_{vdt}\\) is also a dummy variable which takes one for all years following the completion of a new road to village \\(v\\). \\(α_{v}\\) are village fixed effects, while \\(γ_{dt}\\) are the district-year fixed effects. \\(X_{v}\\) controls some baseline characteristics (e.g. forest cover in 2000, total population) and is interacted with year fixed effects \\(V_{t}\\).\nThere is one more step before we run the regressions. In Stata, which the authors used for their regression, reghdfe removed singleton groups automatically. However, the fixest package currently doesn’t possess this functionality, so for now, we will manually remove these observations.\n\n\n\nFinally, we can run our regressions. Following the authors, we test the effect of being awarded a new road and receiving the road on the log forest cover as well as on the average forest cover.\n\n\n\nOur results align with the authors’ findings presented in Table 3 which show that being awarded a road has a negative impact on forest cover (approximately 0.5% loss in the construction period between being awarded a road and its completion), but after the road is constructed, forest cover appears to return. This could incorrectly be interpreted as a positive effect of roads on tree cover if the award term is left out. This determination that rural roads have no effect on forest loss, in combination with the authors’ additional findings of substantial forest loss due to highway construction, have important policy implications for governments considering similar infrastructure expansion. The use of VCF data in this study enabled significant insights, and the potential use cases for VCF data remain numerous.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  }
]
