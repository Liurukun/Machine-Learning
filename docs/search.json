{
  "articles": [
    {
      "path": "about.html",
      "title": "About the class",
      "description": "ECOL6002P, the second semester of an academic year\n\nInstructors: Fanglin Liu\n\nTeaching assistants:\n\nWhen & where: Wednesday and Friday 3,4,5 (9:45-12:10), 3A409\n\nSome notes by Fanglin Liu: https://github.com/flliu315\n",
      "author": [],
      "contents": "\r\nIntroduce of the class\r\nEcology is the study of the relationships between plants/animals, including humans, and their abiotic environment. It seeks to understand the vital connections between living organisms and the world around them.\r\nEcology also provides information about the benefits of ecosystems and how we can use Earth’s resources in ways that leave the environment healthy for future generations.\r\nRead more here\r\nThe distributed sensor networks allow for the acquisition of huge volumes of data on many relevant aspects, ranging from soil and vegetation characteristics, abiotic conditions like weather, to animals’ behavior. The collection of large amounts of data leads to a shift away from frequentist hypothesis testing towards analytics that is more focused on prediction, classification, pattern recognition or anomaly detection. To this end, machine learning techniques are often used, usually by high performance computing.\r\nIn the digital era, researchers are embracing data science, i.e., unifying data processing, statistics, artificial intelligence and their related algorithms to extract knowledge from data. Hence, data science is increasingly becoming an integral part of decision making in many fields, including ecology and wildlife conservation. To keep up with these steps, young scientists and students need to become acquainted with the terms, concepts and methodology of data science, including the integration and pre-processing of data from different sources, and the engineering of informative and discriminating features for creating effective algorithms.\r\nThis class covers the main elements using a data science approach to solve ecological problems. Students will be guided through the main concepts and skills that are required to become a successful data scientist. This class builds upon, and expands, the understanding and skills generated in other courses, and focuses on combining these in an interdisciplinary way to be optimally able to solve ecological problems with a data-driven approach. In the class, students will increase their knowledge and skills that will benefit their future career in academia.\r\nRead more here\r\nPreparing for the class\r\nMake sure to bring your own computer for this class, with access to school wifi and enough battery for three hours of work (or bring your power cable). Any operating system (Windows, Mac, Linux) is OK, as far as you are able to operate it.\r\nInstall the latest version of R and RStudio before you come for the first class - see instructions here.\r\nIf you used R and RStudio before, please make sure that you updated to the version required for this class; instructions on how to update and required R and RStudio versions are here.\r\nIt is essential that we all are using the same version of R and RStudio, to avoid the situation that you will get error messages just because of using outdated program versions.\r\nThis textbook provides many learning resources and reference materials for the class.\r\nExpected learning outcomes\r\nExperience with programming in R is needed to follow and successfully complete this course. The students without prior experience with programming in R are expected to master R programming in the class, including:\r\nMain types of R objects (vector, matrix, data frame, list), reading and exporting data, manipulating data (sorting, merging), creating fully reproducible R script\r\nHow to draw effective scientific figures, including how to choose them and draw them, as well as raster vs vector graphics\r\n\r\nAfter successful completion of this class, students are expected to be able to understand the significance of data science in solving typical ecological problems, including:\r\nunderstand how key features of ecological data influence the selection, training, validation and evaluation of algorithms;\r\nidentify and select machine learning algorithms appropriate to specific ecological problems;\r\napply data science skills (data processing, feature engineering, and machine learning algorithms) to analyse ecological data;\r\ncritically evaluate the results and performance of trained algorithms, and assess the reliability and adequacy of trained algorithms in predicting ecological phenomena;\r\ncreate ecological insight from data using a data science approach.\r\n\r\nFinal evaluation\r\nThe final evaluation consists of three parts:\r\nHomework assignments (30%), see here\r\nActivity in the class, individual work (40%)\r\nFinal presentation (30%), see here\r\n\r\n\r\n\r\n",
      "last_modified": "2023-10-14T23:01:29+08:00"
    },
    {
      "path": "data_manipulation.html",
      "title": "Lesson 2: Data manipluation",
      "description": "In this lesson, you will learn what types of data and how to maniplate data, including data import and export, as well as simple statistic analysis.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Acquring data from R\r\n1.1 The data within R\r\n1.2 The data outside R\r\n\r\n2. Data types and manipulation\r\n2.1 Vectors and maniplation\r\n2.2 lists and manipluation\r\n2.3 Matrics and manipulation\r\n2.4 Arrays and manipulation\r\n2.5 Factors and manipulation\r\n2.6 Data frames and manipulation\r\n\r\n3. Data visualization in R\r\n3.1 Pie charts\r\n3.2 Bar charts\r\n3.3 Boxplots\r\n3.4 Histograms\r\n3.5 Line Charts\r\n3.6 Scatterplots\r\n\r\n\r\n1. Acquring data from R\r\n1.1 The data within R\r\nThere are several ways to find the included datasets in R:\r\nUsing data() to list the datasets of all loaded packages (not only the ones from the datasets package); the datasets are ordered by package\r\nUsing data(package = .packages(all.available = TRUE)) to list all datasets in the available packages on your computer (i.e. also the not-loaded ones)\r\nUsing data(package = “packagename”) to list the datasets built in the package, so data(package = “plyr”) will give the datasets in the plyr package\r\n1.2 The data outside R\r\nIn R you can read data from files stored outside the R environment. You can also write data into files which will be stored and accessed by computers. R can read and write into various file formats like csv, excel, xml etc.\r\nThe file should be present in current working directory so that R can read it. You can set our own directory and read files from there. You can use the getwd() function to check current directory, and also use setwd()functional to set a new working directory.\r\n\r\n[1] \"D:/education-website\"\r\n\r\nA CSV or excel File\r\nThe csv file is a text file in which the values in the columns are separated by a comma. You can use read.csv() function to read it into R.\r\nMicrosoft Excel is the most widely used spreadsheet which stores data in the .xls or .xlsx format. R can read directly from the files using some specific packages, such as xlsx package.\r\n\r\n[1] FALSE\r\n\r\nFrom the web site\r\nMany websites can provide data. For example, WHO provides reports on health and medical information in the form of CSV, txt and XML files. Using R, we can programmatically extract data from websites. Some R packages, such as “RCurl”, “XML”, and “stringr”, are used to connect to the URL’s, identify required links for the data and download them to R environment.\r\n\r\n\r\n\r\nFor example, if you visit the URL weather data and download the CSV files using R for the year 2015.\r\n\r\n\r\n\r\nFrom the databases\r\nThe data is Relational database systems are stored in a normalized format. So, to carry out statistical computing you will need very advanced and complex Sql queries. But R can connect easily to many relational databases like MySql, Oracle, Sql server etc. and fetch records from them as a dataframe. Once the data is available in the R environment, it becomes a normal R data set and can be manipulated or analyzed using packages and functions. Below you will be using MySql as our reference database for connecting to R.\r\nOnce the RMySQL package is installed we create a connection object in R to connect to the database. It takes the username, password, database name and host name as input.\r\n\r\n\r\n\r\nYou can query the database tables in MySql using the dbSendQuery() function. The query executs in MySql and the result is returned using the fetch() function. Finally it is stored as a dataframe in R.\r\n\r\n\r\n\r\nYou can pass any valid select query to get the result.\r\n\r\n\r\n\r\nYou can update the rows in a Mysql table by passing the update query to the dbSendQuery() function.\r\n\r\n\r\n\r\nYou can create tables in the MySql using the dbWriteTable() function. It overwrites the table if it already exists and takes a dataframe as input.\r\n\r\n\r\n\r\nYou can drop the tables in MySql database passing the drop table statement into the dbSendQuery() in the same way as we used it for querying data from tables.\r\n\r\n\r\n\r\n2. Data types and manipulation\r\nGenerally, you may store information of data types like character, integer, floating point, and Boolean, etc. Based on the data type of a variable, the operating system allocates memory and decides what can be stored. There are many types of R-objects. The frequently used ones are:\r\nVectors\r\nLists\r\nMatrices\r\nArrays\r\nFactors\r\nData Frames\r\n2.1 Vectors and maniplation\r\nVectors\r\nIn R the very basic data types are the R-objects called vectors. When creating a vector with more than one element, you should use c() function which means to combine the elements into a vector.\r\n\r\n\r\n# Create a vector.\r\napple <- c('red','green',\"yellow\")\r\nprint(apple)\r\n\r\n[1] \"red\"    \"green\"  \"yellow\"\r\n\r\nVector manipulation\r\nTwo vectors of same length can be added, subtracted, multiplied or divided giving the result as a vector output.\r\n\r\n[1]  7 19  4 13  1 13\r\n[1] -1 -3  4 -3 -1  9\r\n[1] 12 88  0 40  0 22\r\n[1] 0.7500000 0.7272727       Inf 0.6250000 0.0000000 5.5000000\r\n\r\nElements in a vector can be sorted using the sort() function.\r\n\r\n[1]  -9   0   3   4   5   8  11 304\r\n\r\n\r\n[1] 304  11   8   5   4   3   0  -9\r\n\r\n2.2 lists and manipluation\r\nLists\r\nA list is an R-object which can contain many different types of elements inside it like vectors, functions and even another list inside it.\r\n\r\n\r\n# Create a list.\r\nlist1 <- list(c(2,5,3),21.3,sin)\r\n\r\n# Print the list.\r\nprint(list1)\r\n\r\n[[1]]\r\n[1] 2 5 3\r\n\r\n[[2]]\r\n[1] 21.3\r\n\r\n[[3]]\r\nfunction (x)  .Primitive(\"sin\")\r\n\r\nList manipulation\r\nA list can be converted to a vector so that the elements of the vector can be used for further manipulation. All the arithmetic operations on vectors can be applied after the list is converted into vectors. To do this conversion, we use the unlist() function. It takes the list as input and produces a vector.\r\n\r\n[[1]]\r\n[1] 1 2 3 4 5\r\n[[1]]\r\n[1] 10 11 12 13 14\r\n\r\n\r\n[1] 1 2 3 4 5\r\n[1] 10 11 12 13 14\r\n[1] 11 13 15 17 19\r\n\r\n2.3 Matrics and manipulation\r\nMatrices\r\nA matrix is a two-dimensional rectangular R-object. It can be created using a vector input to the matrix function.\r\n\r\n\r\n# Create a matrix.\r\nM = matrix(c('a','a','b','c','b','a'), nrow = 2, ncol = 3, byrow = TRUE)\r\nprint(M)\r\n\r\n     [,1] [,2] [,3]\r\n[1,] \"a\"  \"a\"  \"b\" \r\n[2,] \"c\"  \"b\"  \"a\" \r\n\r\nMatrix manipulation\r\nCreate a matrix taking a vector of numbers as input.\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    3    4    5\r\n[2,]    6    7    8\r\n[3,]    9   10   11\r\n[4,]   12   13   14\r\n     [,1] [,2] [,3]\r\n[1,]    3    7   11\r\n[2,]    4    8   12\r\n[3,]    5    9   13\r\n[4,]    6   10   14\r\n     col1 col2 col3\r\nrow1    3    4    5\r\nrow2    6    7    8\r\nrow3    9   10   11\r\nrow4   12   13   14\r\n\r\nVarious mathematical operations are performed on the matrices using the R operators. The result of the operation is also a matrix.\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    3   -1    2\r\n[2,]    9    4    6\r\n     [,1] [,2] [,3]\r\n[1,]    5    0    3\r\n[2,]    2    9    4\r\nResult of addition \r\n     [,1] [,2] [,3]\r\n[1,]    8   -1    5\r\n[2,]   11   13   10\r\nResult of subtraction \r\n     [,1] [,2] [,3]\r\n[1,]   -2   -1   -1\r\n[2,]    7   -5    2\r\n\r\n2.4 Arrays and manipulation\r\nArrays\r\nWhile matrices are confined to two dimensions, arrays can be of any number of dimensions. The array function takes a dim attribute which creates the required number of dimension. Below is an example array with two elements which are 3x3 matrices each. Arrays can store data in more than two dimensions.\r\n\r\n\r\n# Create an array.\r\na <- array(c('green','yellow'),dim = c(3,3,2)) #  2 matrices each with 3 rows and 3 columns\r\nprint(a)\r\n\r\n, , 1\r\n\r\n     [,1]     [,2]     [,3]    \r\n[1,] \"green\"  \"yellow\" \"green\" \r\n[2,] \"yellow\" \"green\"  \"yellow\"\r\n[3,] \"green\"  \"yellow\" \"green\" \r\n\r\n, , 2\r\n\r\n     [,1]     [,2]     [,3]    \r\n[1,] \"yellow\" \"green\"  \"yellow\"\r\n[2,] \"green\"  \"yellow\" \"green\" \r\n[3,] \"yellow\" \"green\"  \"yellow\"\r\n\r\nArray Manipulation\r\nArrays are the R-objects that can store data in more than two dimensions. If we create an array of dimension (2, 3, 4) then it creates 4 rectangular matrices each with 2 rows and 3 columns. Arrays can store only data type.\r\nAn array is created using the array() function. It takes vectors as input and uses the values in the dim parameter to create an array.\r\n\r\n, , 1\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    5   10   13\r\n[2,]    9   11   14\r\n[3,]    3   12   15\r\n\r\n, , 2\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    5   10   13\r\n[2,]    9   11   14\r\n[3,]    3   12   15\r\n\r\n2.5 Factors and manipulation\r\nFactors\r\nFactors are the R-objects which are created using a vector. It stores a vector along with the distinct values of the elements in the vector as labels. The labels are always character irrespective of whether it is numeric or character or Boolean etc. in the input vector. They are useful in statistical modeling.\r\nFactors are created using the factor() function. The nlevels() functions gives the count of levels.\r\n\r\n\r\n# Create a vector\r\napple_colors <- c('green','green','yellow','red','red','red','green')\r\n\r\n# Create a factor object\r\nfactor_apple <- factor(apple_colors)\r\n\r\n# Print the factor\r\nprint(factor_apple)\r\n\r\n[1] green  green  yellow red    red    red    green \r\nLevels: green red yellow\r\n\r\nprint(nlevels(factor_apple))\r\n\r\n[1] 3\r\n\r\nFactor Manipulation\r\nFactors are the R-objects that are used to categorize the data and store it as levels. They can store both strings and integers. They are useful in the columns that have a limited number of unique values. Like “Male,”Female” and True, False etc. They are useful in data analysis for statistical modeling.\r\n\r\n [1] \"East\"  \"West\"  \"East\"  \"North\" \"North\" \"East\"  \"West\"  \"West\" \r\n [9] \"West\"  \"East\"  \"North\"\r\n[1] FALSE\r\n [1] East  West  East  North North East  West  West  West  East  North\r\nLevels: East North West\r\n[1] TRUE\r\n\r\n2.6 Data frames and manipulation\r\nData Frames\r\nData frames are tabular data objects. Unlike a matrix in data frame each column can contain different modes of data. The first column can be numeric while the second column can be character and third column can be logical. It is a list of vectors of equal length.\r\nData Frames are created using the data.frame() function.\r\n\r\n\r\n# Create the data frame.\r\nBMI <-   data.frame(\r\n   gender = c(\"Male\", \"Male\",\"Female\"), \r\n   height = c(152, 171.5, 165), \r\n   weight = c(81,93, 78),\r\n   Age = c(42,38,26)\r\n)\r\nprint(BMI)\r\n\r\n  gender height weight Age\r\n1   Male  152.0     81  42\r\n2   Male  171.5     93  38\r\n3 Female  165.0     78  26\r\n\r\nData frame Manipulation\r\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. You can use data.frame() function to create a dataframe.\r\n\r\n  emp_id emp_name salary start_date\r\n1      1     Rick 623.30 2012-01-01\r\n2      2      Dan 515.20 2013-09-23\r\n3      3 Michelle 611.00 2014-11-15\r\n4      4     Ryan 729.00 2014-05-11\r\n5      5     Gary 843.25 2015-03-27\r\n\r\nThe structure of the data frame can be seen by using str() function.\r\n\r\n'data.frame':   5 obs. of  4 variables:\r\n $ emp_id    : int  1 2 3 4 5\r\n $ emp_name  : chr  \"Rick\" \"Dan\" \"Michelle\" \"Ryan\" ...\r\n $ salary    : num  623 515 611 729 843\r\n $ start_date: Date, format: \"2012-01-01\" ...\r\n\r\nThe statistical summary and nature of the data can be obtained by applying summary() function.\r\n\r\n     emp_id    emp_name             salary        start_date        \r\n Min.   :1   Length:5           Min.   :515.2   Min.   :2012-01-01  \r\n 1st Qu.:2   Class :character   1st Qu.:611.0   1st Qu.:2013-09-23  \r\n Median :3   Mode  :character   Median :623.3   Median :2014-05-11  \r\n Mean   :3                      Mean   :664.4   Mean   :2014-01-14  \r\n 3rd Qu.:4                      3rd Qu.:729.0   3rd Qu.:2014-11-15  \r\n Max.   :5                      Max.   :843.2   Max.   :2015-03-27  \r\n\r\nExtract specific column from a dataframe using column name.\r\n\r\n  emp.data.emp_name emp.data.salary\r\n1              Rick          623.30\r\n2               Dan          515.20\r\n3          Michelle          611.00\r\n4              Ryan          729.00\r\n5              Gary          843.25\r\n\r\nExtract the first two rows and then all columns.\r\n\r\n  emp_id emp_name salary start_date\r\n1      1     Rick  623.3 2012-01-01\r\n2      2      Dan  515.2 2013-09-23\r\n\r\nExtract 3rd and 5th row with 2nd and 4th column.\r\n\r\n  emp_name start_date\r\n3 Michelle 2014-11-15\r\n5     Gary 2015-03-27\r\n\r\nA dataframe can be expanded by adding columns and rows.\r\n\r\n  emp_id emp_name salary start_date       dept\r\n1      1     Rick 623.30 2012-01-01         IT\r\n2      2      Dan 515.20 2013-09-23 Operations\r\n3      3 Michelle 611.00 2014-11-15         IT\r\n4      4     Ryan 729.00 2014-05-11         HR\r\n5      5     Gary 843.25 2015-03-27    Finance\r\n\r\nTo add more rows permanently to an existing data frame, we need to bring in the new rows in the same structure as the existing data frame and use the rbind() function.\r\n\r\n  emp_id emp_name salary start_date       dept\r\n1      1     Rick 623.30 2012-01-01         IT\r\n2      2      Dan 515.20 2013-09-23 Operations\r\n3      3 Michelle 611.00 2014-11-15         IT\r\n4      4     Ryan 729.00 2014-05-11         HR\r\n5      5     Gary 843.25 2015-03-27    Finance\r\n6      6    Rasmi 578.00 2013-05-21         IT\r\n7      7   Pranab 722.50 2013-07-30 Operations\r\n8      8    Tusar 632.80 2014-06-17   Fianance\r\n\r\nData reshape\r\nData Reshaping in R is about changing the way data is organized into rows and columns. Most of the time data processing in R is done by taking the input data as a dataframe. It is easy to extract data from the rows and columns of a data frame but there are situations when we need the dataframe in a format that is different from format in which we received it. R has many functions to split, merge and change the rows to columns and vice-versa in a data frame.\r\nYou can join multiple vectors to create a data frame using the cbind()function. Also we can merge two data frames using rbind() function.\r\n\r\n# # # # The First data frame\r\n     city       state zipcode\r\n[1,] \"Tampa\"    \"FL\"  \"33602\"\r\n[2,] \"Seattle\"  \"WA\"  \"98104\"\r\n[3,] \"Hartford\" \"CT\"  \"6161\" \r\n[4,] \"Denver\"   \"CO\"  \"80294\"\r\n# # # The Second data frame\r\n       city state zipcode\r\n1     Lowry    CO   80230\r\n2 Charlotte    FL   33949\r\n# # # The combined data frame\r\n       city state zipcode\r\n1     Tampa    FL   33602\r\n2   Seattle    WA   98104\r\n3  Hartford    CT    6161\r\n4    Denver    CO   80294\r\n5     Lowry    CO   80230\r\n6 Charlotte    FL   33949\r\n\r\nYou can merge two dataframes by using the merge() function. The data frames must have same column names on which the merging happens.\r\nIn the example below, you consider the data sets about Diabetes in the MASS package. we merge the two data sets based on the values of blood pressure (“bp”) and body mass index (“bmi”). To choose these two columns for merging, the records where values of two variables match in both data sets are combined together to form a single data frame.\r\n\r\n   bp  bmi npreg.x glu.x skin.x ped.x age.x type.x npreg.y glu.y\r\n1  60 33.8       1   117     23 0.466    27     No       2   125\r\n2  64 29.7       2    75     24 0.370    33     No       2   100\r\n3  64 31.2       5   189     33 0.583    29    Yes       3   158\r\n4  64 33.2       4   117     27 0.230    24     No       1    96\r\n5  66 38.1       3   115     39 0.150    28     No       1   114\r\n6  68 38.5       2   100     25 0.324    26     No       7   129\r\n7  70 27.4       1   116     28 0.204    21     No       0   124\r\n8  70 33.1       4    91     32 0.446    22     No       9   123\r\n9  70 35.4       9   124     33 0.282    34     No       6   134\r\n10 72 25.6       1   157     21 0.123    24     No       4    99\r\n11 72 37.7       5    95     33 0.370    27     No       6   103\r\n12 74 25.9       9   134     33 0.460    81     No       8   126\r\n13 74 25.9       1    95     21 0.673    36     No       8   126\r\n14 78 27.6       5    88     30 0.258    37     No       6   125\r\n15 78 27.6      10   122     31 0.512    45     No       6   125\r\n16 78 39.4       2   112     50 0.175    24     No       4   112\r\n17 88 34.5       1   117     24 0.403    40    Yes       4   127\r\n   skin.y ped.y age.y type.y\r\n1      20 0.088    31     No\r\n2      23 0.368    21     No\r\n3      13 0.295    24     No\r\n4      27 0.289    21     No\r\n5      36 0.289    21     No\r\n6      49 0.439    43    Yes\r\n7      20 0.254    36    Yes\r\n8      44 0.374    40     No\r\n9      23 0.542    29    Yes\r\n10     17 0.294    28     No\r\n11     32 0.324    55     No\r\n12     38 0.162    39     No\r\n13     38 0.162    39     No\r\n14     31 0.565    49    Yes\r\n15     31 0.565    49    Yes\r\n16     40 0.236    38     No\r\n17     11 0.598    28     No\r\n[1] 17\r\n\r\nOne of the most interesting aspects of R is about changing the shape of the data in multiple steps to get a desired shape. The functions used to do this are called melt() and cast(). We consider the dataset called ships in the MASS package.\r\n\r\n\r\n\r\nYou can cast the molten data into a new form where the aggregate of each type of ship for each year is created. It is done using the cast() function.\r\n3. Data visualization in R\r\n3.1 Pie charts\r\nIn R the pie chart is created using the pie() function which takes positive numbers as a vector input.\r\n\r\n\r\n\r\n3.2 Bar charts\r\nThe below script will create and save the bar chart in the current R working directory.\r\n\r\n\r\n\r\n3.3 Boxplots\r\nBoxplots are created in R by using the boxplot() function.\r\n\r\n\r\n\r\n3.4 Histograms\r\nR creates histogram using hist() function. This function takes a vector as an input and uses some more parameters to plot histograms.\r\n\r\n\r\n\r\n3.5 Line Charts\r\nA line chart is a graph that connects a series of points by drawing line segments between them. The plot() function is used to create the line graph.\r\n\r\n\r\n\r\n3.6 Scatterplots\r\nScatterplots show many points plotted in the Cartesian plane. Each point represents the values of two variables. One variable is chosen in the horizontal axis and another in the vertical axis. The simple scatterplot is created using the plot() function.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-10-14T23:01:32+08:00"
    },
    {
      "path": "general_overview.html",
      "title": "Overview: From data to models",
      "description": "The growth of Earth observation systems together with innovative data processing and analysis provide opportunities to monitor spatial–temporal changes of biodiversity and to evaluate the effects of human impact on ecological processes. In this section we will give a general introduce about the ecological data sources, and the methods for data pre-process and modelling with machine learning.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Ecodata sources\r\n2. Exploratory Data Analysis\r\n3. Machine learning models\r\n\r\n1. Ecodata sources\r\n1.1 General introduce\r\nThere are lots of mix databases, websites, and tools to find interdisciplinary data for your ecological researches. For example, this iDigBIo website provides natural history museum and fossil records about specimens all over the world.\r\nThere is a growing need for baseline data to evaluate the changes in biodiversity at the community level and to distinguish these change that can be attributed to external factors, such as anthropogenic activities, from underlying natural change. Here lists three aspects of global long-term datasets that benefit to macroecological reseraches.\r\n1.2 The data of biodiversity\r\nThe changes of species abundance and distribution can be tracked using a long-term dataset, which is simply information on the variety, and ideally the abundance, of species (or other taxonomic units) at one or more locations at a number of points in time. Such datasets can be retrievered from many repositories:\r\nGBIF: This web provides an international network and data infrastructure funded by the world’s governments and aimed at providing anyone, anywhere, open access to data about all types of life on Earth.\r\nFragSAD: This dataset collects species abundance and distribution in habitat fragments from temperate forests and grasslands from all continents except Antarctica. The data include invertebrates, plants, birds, mammals, and reptiles and amphibians.\r\nBIEN: This web brings together many types of data needed to address several pressing ecological and global change questions. You access to the data of all plant species in the New World, including: 1) plant observations from herbarium and plots, and species geographic distribution maps; 2) vegetation and plot inventories and plant traits; 3) cross-continent, continent, and country-level species lists.\r\n1.3 Eco-environmental data\r\nThe general methods of environmental data has been derived from maps, which include GIS data and remote sensing images for investigation of environmental matrices, such as soil, water, vegetation, and air, etc., and a better understanding of ecological and environmental interactions to support the development of sustainable solutions.\r\nGIS data: This page provides a categorised list of links to over 500 sites providing freely available geographic datasets - all ready for loading into a Geographic Information System. The data include Boundaries, Elevation, Weather/climate, Hydrology, Snow/Ice, and Land Cover, etc.\r\nRemote sensing images: NASA has a wide variety of satellite and aerial remote sensing datasets and interactive maps at their Earthdata Search and other sites. In addition, Remote sensing of light emissions offers a unique perspective for investigations into some human behaviors. Day/Night Band (DNB) data are often used for estimating population, assessing electrification of remote areas, monitoring disasters and conflict, and understanding biological impacts of increased light pollution.\r\n1.4 Citizen Science\r\nCitizen Science can be used as a methodology where public volunteers help in collecting and classifying data, which are used extensively in studies of biodiversity and pollution (For details, see box1).\r\n\r\n\r\n\r\nBox1: Citizen science \r\n\r\n\r\nCitizen science, a two-way cooperation between the scientific and public communities in the long-term monitoring programs, is not new. One of the oldest citizen science programs is organised by the National Audubon Society. In the winter of 2000–2001, the participants came up to 52471 people in 1823 places involved 17 countries. Advances in electronic recording and communication systems via the web and mobile phones have led to a resurgence with projects like ProjectBudburst. Involving unpaid volunteers has the advantages of being economic, can extend the geographic range of study sites and the frequency of visits. Exploring the citizen science data collection can be optimized for biodiversity research.\r\n\r\n\r\n\r\n\r\n\r\n2. Exploratory Data Analysis\r\n2.1 Data integration\r\nVarious data sources provide information on the study system components that operate at different scales. Macro-ecology is the study of ecological patterns and processes at broad spatio-temporal scales. Such broad- and multi-scale research questions need combining disparate datasets from different data sources.\r\nThe most common challenges for data integration were: (1) mismatches in spatial or temporal scale of data sources, (2) differences in the quantity and/or information content of data sources, (3) sampling biases, and (4) optimizating model development and assessment. One of the task of data exploratory analysis (EDA) is to integrate data correctly.\r\n2.2 Data pre-process\r\nSome datasets contain missing data. You should properly handle them, either exclude them from your analysis or delete them. Also, datasets may include some outliers and features of the data that might be unexpected. You must understand where outliers occur and how variables are related, which can help designing statistical analyses that yield meaningful results.\r\n2.3 Statistics analysis\r\nAnother task of EDA is to identify general patterns in the data. The patterns should be revealed before further analysis. You can use descriptive statistics to complete such work. Scatterplots and correlation coefficients can provide useful information on relationships between pairs of variables. But when analyzing numerous variables, basic methods of multivariate visualization can provide greater insights. Mapping data also is critical for understanding spatial relationships among samples.\r\n3. Machine learning models\r\n3.1 Decision tree\r\nEcological data are specifically known to be non-linear and highly dimensional with intense interaction effects. To make statistic methods work, which assume linearity, researchers cope in various ways, including data transformations and break up systems into bits with fewer complexes. However, machine learning (ML) techniques have been shown to outperform traditional statistical approaches in solving the problems.\r\nLots of ML algorithms have been developed, including unsupervised ML and supervised ML. Unsupervised ML is often used to identify patterns in data. Supervised ML is used to train machine to learn from other examples for predicting a categorical outcome (classification) or a numeric outcome (regression), or inferring the relationships between the outcome and its explanatory variables.. Thus, supervised ML is widely used to make predictions and inferences\r\nThe widely used ML in ecology is tree-base algorithms. For these methods, a tree is built by iteratively splitting the data set based on a rule that results in the divided groups being more homogeneous than the group before.\r\nThe basic idea behind the algorithm is to find a point in the independent variable to split the data-set into 2 parts, so that the mean squared error (mse) is the minimized at that point. The algorithm does this in a repetitive fashion and forms a tree-like structure. Let’s consider a dataset where we have 2 variables, as shown below:\r\n\r\n\r\n\r\nThe first step is to sort X from min to max, and use the median of the first 2 points of X (i.e. (84 + 100)/2 = 92) to divide the dataset into 2 parts (Part A and Part B) , separated by x < 92 and X ≥ 92.\r\nThe averages of all Y values in Part A and those of all Y values in Part B are used to be the output of the decision tree for x < 92 and x ≥ 92 respectively. Based on the predicted and original values, calculate the mse according to the formula:\r\n\\[mse = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i-\\hat{y_i})\\]\r\nWe repeat the procedure using for the second 2 points of X (i.e. (100 + 180)/2 = 140), and split the dataset into part A (X < 140) and part B (X ≥ 140) as predicted output. Based on the 2 parts, we calculate mse as step 1. Similarly, mse is repeatedly calcualted for the third 2 points of X, the fourth 2 points of X, …, till \\((n-1)^th\\) 2 points of X.\r\nNow that we have n-1 mse, and choose the point, at which the mse is smallest. In this case, the point is x = 2115. Thus, we can split the dataset into the x < 2115 and x ≥ 2115 parts. The first split can be obtained using rpart package with maxdepth = 1 as follows:\r\n\r\n\r\n\r\nWe can improve the tree by adding cross-validation like this:\r\n\r\n\r\n\r\nYou then obtain decision boundary by running the codes:\r\n\r\n\r\n\r\nThe left (x < 2115) and right (x ≥ 2115) parts are further recursively exposed to the same algorithm for the following split. When reducing mse, the tree can recursively split the data-set into a large number of subsets to the point where a set contains only one. Finally, we could obtain a piecewise function like below:\r\n\r\n\r\n\r\n3.2 Random forest\r\nRandom Forest is a relatively new tree-based method that fits a user-selected number of trees to a data set and then combines the predictions from all trees. The Random Forest algorithm creates a tree for a subsample of the data set. At every decision only a randomly selected subset of variables are used for the partitioning. The predicted output of an observation in the final tree is calculated by the majority of the predictions for that observation in all trees with ties split randomly.\r\nEnsemble tree-based methods, especially Random Forest, have been shown to outperform statistical and other ML methods in ecology applications. They can cope with small sample sizes, mixed data types, and missing data. The single-tree methods are fast to calculate and the results are easy to interpret, but they are susceptible to overfitting and need “pruning” of terminal nodes. The ensemble methods are computationally expensive, but resist overfitting. Random Forest can provide measures of relative variable importance and the data point similarity that can be useful in other analyses, but can be clouded by correlations between independent variables.\r\n3.3 Artificial Neural Networks\r\nMultiple percetion machine\r\nArtificial Neural Network (ANN) is a ML approach inspired by the way neurological systems process information. There are many ANN algorithms, which are supervised or unsupervised learners, but only a few are typically used in ecology. An ANN has three parts: 1) the input layer, 2) the hidden layer, and 3) the output layer.\r\nEach layer have several “neurons”. Each neuron is connected to the other neurons in the neighboring layer, but not the neurons in the same or in non-adjacent layers. The input layer contains a neuron for an independent variable. The output layer can have one neuron (for binary or continuous output) or more (for categorical output). The number of neurons in the hidden layer can be changed by the user to optimize the trade-off between overfitting and variance. Too many neurons in this layer can lead to overfitting. Each neuron has an activity level and each connection has a weight. The activity level of the input neurons are set by the value of the independent variable. Training the ANN involves an algorithmic search for an optimal set of connection weights that produces an output value with a small error relative to the observed value. Performance can be sensitive to initial connection weights and the number of hidden neurons, so multiple networks should be processed while varying these parameters.\r\nANN can be a powerful modeling tool when the underlying relationships are unknown and the data are imprecise and noisy. Interpretation of the ANN can be difficult and ANNs are often referred to as a “black box” method. Many ANNs mimic standard statistical methods, so a good practice while using ANNs is to also include a rigorous suite of validation tests and a general linear model for comparison.\r\nDeep learning\r\nA neural network comprises an input layer, a hidden layer, and an output layer. Deep learning, on the other hand, is made up of several hidden layers of neural networks that perform complex operations on massive amounts of structured and unstructured data.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-10-14T23:01:33+08:00"
    },
    {
      "path": "getting_started.html",
      "title": "Working R with github",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Using Rstudio to interact with R\r\n1.1 Installing R and RStudio\r\n1.2 Updating R and RStudio\r\n1.3 Setting up RStudio Cloud\r\n1.4 Installing R packages\r\n\r\n2. Creating a R project\r\n2.1 Using a project to organize your work\r\n2.2 Working directory structure\r\n2.3 General settings\r\n2.4 Automatically running\r\n\r\n3. Connecting a R project to Github\r\n3.1 Checkinf that RStudio can find Git\r\n3.2 Adding a new R project to GitHub\r\n3.3 Adding an existing R project to GitHub\r\n\r\n4. Writing R code\r\n4.1 Using script\r\n4.2 Using rmarkdown\r\n4.3 Objects and operators\r\n\r\n\r\nYou need to install R and RStudion on your computers. We will go through the following steps to install two free programs. Also, you’ll understand basic R commands, and the RStudio interface with R in order to start programming.\r\n1. Using Rstudio to interact with R\r\n1.1 Installing R and RStudio\r\nR and RStudio are two separate pieces of software:\r\nR is a programming language that is especially powerful for data exploration, visualization, and statistical analysis\r\nRStudio is an integrated development environment (IDE) that makes using R easier\r\nIn this class we will use RStudio to interact with R. The following steps outline a simple and effective process for installing R and RStudio on your computers.\r\nWindows\r\nDownload R from CRAN. Run the .exe file that was just downloaded.\r\nNext to RStudio to download the RStudio Installer for Windows, and double click the file to install it.\r\nA good practice to install RTools, corresponding to R version, for building R and R packages from source on Windows.\r\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\r\nMacOS\r\nGo to CRAN, and select the <mark style=“background-color: #F0F0F0”.pkg file for the latest R version. Double click on the downloaded file to install R.\r\nA good idea to install XQuartz (needed by some packages).\r\nGo to RStudio to download the RStudio Installer for MacOS. Double click the file to install RStudio.\r\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\r\nLinux\r\nFollow the instructions from CRAN, run\r\nsudo apt-get install r-base for Debian/Ubuntu and sudo yum install R for Fedora, but the installed versions of R are usually out of date.\r\nGo to RStudio, select the version that matches your distribution to run sudo dpkg -i rstudio-YYYY.MM.X-ZZZ-amd64.deb for Debian/Ubuntu at the terminal.\r\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\r\nIf R and RStudio are installed, determine whether your R and RStudio versions are necessary to up to date.\r\n1.2 Updating R and RStudio\r\nUpdating R\r\nOpening RStudio, your R version will be printed in the console. Alternatively, you can type sessionInfo() into the console. If the R version is 4.0.0 or later, don’t need to update R for this class. If the version of R is older than that, download and install the latest version of R.\r\nIt is not necessary to remove old versions of R from your system,\r\nbut if you wish to do so you can check\r\nHow do I uninstall R?.\r\nNormally, your old code should still work after updating your R version. But if breaking changes happen, it is useful to know that you have multiple versions of R installed in parallel and that you can switch between them in RStudio by going to Tools > Global Options > General > Basic\r\nAfter installing a new version of R, you will have to reinstall all your packages with the new one. For Windows, there is a package called installr that can help you with upgrading your R version and migrate your package library.\r\nUpdating RStudio\r\nTo update RStudio, open RStudio and click on\r\nHelp > Check for Updates. If a new version is available, RStudio will automatically notify you every once in a while.\r\n1.3 Setting up RStudio Cloud\r\nIf it isn’t feasible to install R and RStudio Desktop on your computer, you can use RStudio Cloud and run R in an online browser window. For this purpose, you need to sign up and create a new account of RStudio Cloud.\r\n\r\n\r\nSetting up\r\nGo to https://rstudio.cloud/.\r\nSign up with an email and create a password of your choice.\r\nSign In.\r\nNavigating\r\nOnce log in, you should look for a few things. On the left hand side, you should see column that displays your “Spaces”, you can check for Learning, as well as some additional info on the system status and terms and conditions.\r\n\r\nOn the right hand side, you should see a small chart showing your Account Usage. This is what you want to keep track of. Depending on how much time you spend actually running code, your time will vary, but the standard free account provides 15 hours per month.\r\nGetting Setup\r\nClick on New Project, and wait a second for things to initialize.\r\nName your project.\r\nCopy and install the packages we’ll need.\r\nCheck everything is installed…you should get TRUE’s if everything worked.\r\n1.4 Installing R packages\r\nMost of the work in R is done by basic functions, which are wrapped into packages. Except for basic functions, R packages have build-in sample data sets.\r\nBy default, a set of R packages is installed during R installation. There also are many packages that are needed to install later from the central repositories like CRAN or Bioconductor, as well as developer repositories like R-Forge(https://r-forge.r-project.org/) or GitHub.\r\nInstalling R packages\r\nTo install R packages, open RStudio and copy and paste the following command into the console window, then execute the command.\r\nInstalling from CRAN\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\ninstall.packages(\"RSQLite\")\r\n# Alternatively\r\ninstall.packages(c(\"tidyverse\", \"RSQLite\"))\r\n\r\n\r\nInstalling from GitHub\r\n\r\n\r\ninstall.packages (\"devtools\")\r\ndevtools::install_github (\"grssnbchr/hexbin\")\r\n\r\n\r\nInstalling from R-Forge\r\n\r\n\r\n#install.packages(\"patchwork\", repos=\"http://R-Forge.R-project.org\")\r\n\r\n\r\nAlternatively, you can install the packages using RStudio interface by going to Tools > Install Packages and typing the names of the packages separated by a comma.\r\nWhen the installation has finished, you can try to load the\r\npackages by pasting the following code into the console:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(hexbin)\r\nlibrary(patchwork)\r\nlibrary(RSQLite)\r\n\r\n\r\nUpdating R packages\r\nIt is recommended to keep your R version and all packages up to date. To update the packages that you have installed, click Update in the Packages tab in the bottom right panel of RStudio, or go to Tools > Check for Package Updates....\r\n2. Creating a R project\r\n2.1 Using a project to organize your work\r\nWhen using R several years ago, it is ususally to first set a working directory using setwd(), which takes an absolute file path as an input and sets it as the current working directory of the R process, and to then use getwd() for finding out whether the current working directory is correctly set. The problem with this approach is that since setwd() relies on an absolute file path. This makes the links break very easily, and very difficult to share your analysis with others.\r\nAn RStudio project solves the problem of ‘fragile’ file paths by making file paths relative. An RStudio projects is the file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through .Rproj, the current working directory points to the root folder where that .Rproj file is saved.\r\nThis .Rproj file can be created by going to File > New Project… in RStudio, which is then link a specified folder or directory that is stand-alone and portable. You can reading data from or writing data to files within this directory, except for cases where your analysis requires interacting with an Internet data source, such as web-scraping. When opening an existing project, you will open the .Rproj file and subsequently open R scripts (extensions with .R) from the RStudio session, rather than going to the R scripts to open them. There are lots of documents on RStudio projects, which have detail information on .RData and .Rhistory files.\r\n2.2 Working directory structure\r\nThis directory structure ‘template’ can provide a good starting point for organizing projects if workflow is new to you. However, different projects will have different needs, and thus one should think about what is needed and what will happen while setting up the working directory structure. A template of an R project like this:\r\nThe data folder\r\nThe data folder is is the subfolder where data are stored. They include any source files, such as SPSS, Excel/CSV or .RDS., and some generated ones. Someone would like to split the subfolder into three parts:\r\na data/raw/ folder, which is usually is symlinked to a folder that is read-only to the R user\r\na data/temp/ folder, which contains temp data\r\na data/output/ folder, if warranted\r\nThe src folder\r\nThis folder stores R script files (with the extension .R) and Rmarkdown ones (with the extension .Rmd) for data analysis and visualization. There are three types of R scripts:\r\nScripts: It is helpful to separate multiple scripts for different tasks on a single data set. Most analysis R scripts are saved here. But the key analysis script is better to be stored lonely.\r\nFunctions: It is optional whether you have your custom functions saved in a separate sub-folder. If you want to re-use a function that you remember you’ve written in a particular project, I can at a quick glance browse all the functions you’ve written for that project. Saving functions separately accompanies a workflow where you use source() to read functions into the ‘main analysis script’, rather than having it together with main analysis.\r\nRMarkdown: RMarkdown files are a special case, as they work slightly differently to .R files in terms of file paths, i.e. they behave like mini projects of their own, where the default working directory is where the Rmd file is saved. To save RMarkdown files in this set up, it’s recommended that you use the {here} package and its workflow. Alternatively, you can run knitr::opts_knit$set(root.dir = “../”) in your setup chunk so that the working directory is set in the root directory rather than another sub-folder where the RMarkdown file is saved (less ideal than using here package and its workflow). You briefly discussed a directory structure for combining multiple RMarkdown files into a single long RMarkdown document.\r\nThe output folder\r\nIn Output folder, save all your outputs here, including plots, HTML, and data exports.\r\nHaving this Output folder helps others identify what files are outputs of the code, as opposed to source files that were used to produce the analysis.\r\nWhat you have set up as the sub-folders don’t matter too much, as long as they’re sensible. Normally, the output folders structure as output/ figs/ or output/ plots/, rather than top-tier folders. They also include word or PDF report results. You may decide to set up the sub-folders so that they align with the analysis rather than type of file export.\r\nThe timed_fn() function from my package surveytoolbox (available on GitHub) helps create timestamps for file names, which you use often to ensure that you don’t lose work when you am iterating analysis.\r\n2.3 General settings\r\nThe requirement.R file: In this case, you should have a requirements.R file. Running it for fundamental settings you like to rely on, such as setting the locale appropriately. It also includes a CRAN install check script, although the Packrat package is advised to use.\r\nUsing keyring package: The keyring package, which interacts with OS X’s keychain, Windows’s Credential Store and the Secret Service API on Linux (where supported)\r\nUsing environment variables: Using environment variables to hold certain secrets has become extremely popular especially for Dockerised implementations of R code, as envvars can be very easily set using Docker. If you create an envfile called .Renviron in the working directory, it will store values in the environment.\r\nUsing a .gitignored secret file: config is a package that allows you to keep a range of configuration settings outside your code, in a YAML file, then retrieve them. You can create a default configuration for an API. A dedicated secrets file is a better place for credentials than a config file, as this file can then be wholesale .gitignore.\r\n2.4 Automatically running\r\nThere is a main runner script or potentially a small number. These go through scripts in a sequence. It is a sensible idea in such a case to establish sequential subfolders or sequentially numbered scripts that are executed in sequence. Typically, this model performs better if there are at most a handful distinct pipelines.\r\n3. Connecting a R project to Github\r\n3.1 Checkinf that RStudio can find Git\r\nThe first task is to ensure that Git can be located by RStudio on your machine. To do this, open RStudio and go to Tools > Global Options > Git/SVN.\r\nUnder “Git executable”, you should be able to see a path to Git. Take ubuntu as example, it will be in /usr/bin/git.\r\nIf Git is not in this location or you want to check where the Git executable path is, open the Command Prompt in ubuntu or Windows terminal. Type where git to reveal the Git executable file path.\r\nIf it doesn’t match the dialogue box in RStudio, click on “Browse…” and navigate to your Git executable file. Once complete, press “OK”.\r\n3.2 Adding a new R project to GitHub\r\nIf you want to start a new RStudio project and have it backed-up on GitHub, follow the following steps:\r\nFirstly, create an acount of Github, and a new repository on the GitHub website. You can choose public or private for your visibility setting\r\nNext, open up RStudio and go to File > New Project… > Version Control, and click on the “Git” option\r\nFill in the URL of the new GitHub repository that you just created in the “Repository URL”“Project Directory Name” will auto-fill. Click on “Open in New Session” and then click on “Create Project”. A new project window will open up in RStudio containing your new project. You will notice that it will contain some files under the “Files” window including .gitignore, .Rproj and README.md. The last file was pulled-down from your GitHub repository\r\nNext, to demonstrate how changes can be saved, you will create a new script file and add some code. This will then be saved locally. Following that, you will “push” my changes to GitHub so that my changes are also saved remotely. To do this, go to File > New File > R Script in RStudio. Write an R script and save it\r\nThis has saved your work to your computer, but not to GitHub. For saving to GitHub, go to the “Git” tab in the upper right pane. Check the “Staged” box for any files whose existence or modifications you want to commit\r\nClick on “Commit” and a new dialogue box will open. Under “Commit Message”, add a brief description of the changes that you have made\r\nClick on “Commit”. A Git Commit dialogue box will be displayed showing that the files are committed to GitHub. You may close this second window by clicking “Close”\r\nComplete the final step by pressing Push. This will upload the R files to GitHub. You will see a dialogue box come up confirming this in the form of a string followed by HEAD -> main\r\n3.3 Adding an existing R project to GitHub\r\nIf you have an existing project in RStudio and decide later that maintaining version control in GitHub would be a good idea, follow the steps:\r\nFirstly, create a new repository on GitHub via the website, and choose a repository name like above\r\nNext, open the Command Prompt (or ubuntu terminal) and go to the folder that contains your existing R project, then substitute the URL for your GitHub repository address for remote add origin and push by performing the following codes\r\n\r\n\r\n# git remote add origin https://github.com/yourusername/yourrepo\r\n# git pull origin main\r\n# git push -u origin main\r\n\r\n\r\n4. Writing R code\r\n4.1 Using script\r\nUsing R understand language to tell R what and how do the thing that you want. We refer this discription to “script”.\r\nCreating R script\r\nA script is simply a text file that contains a set of commands and comments. It can be saved and reused later. It can also be edited so you can execute a modified version of the commands.\r\nTo create a new script in RStudio, you can open a new empty script by clicking the New File–>New File Menu–>R Script. The script editor opens with an empty script, which is ready for text entry. Here is an example to familiarize you with the Script Editor interface.\r\n\r\nSaving R script\r\nYou can save your script by clicking on the Save icon at the top of the Script Editor panel. When you do that, a Save File dialog will open.\r\nThe default script name is Untitled.R. The Untitled part is highlighted. You will save this script as First script.R. Start typing First script. RStudio overwrites the highlighted default name with your new name, leaving the .R file extension.\r\nNotice that RStudio will save your script to your current working folder. Press the Save button and your script is saved to your working folder. Notice that the name in the file at the top of the Script Editor panel now shows your saved script file name.\r\nWhile it is not necessary to use an .R file extension for your R scripts, it does make it easier for RStudio to work with them if your use this file extension.\r\nOpening R script\r\nClick on the Open an existing file icon in the RStudio toolbar. A choose file dialog will open. Select the R script you want to open and click the Open button. Your script will open in the Script Editor panel with the script name in an editor tab.\r\nCommenting R script\r\nIn scripts, it can be very useful to save a bit of text which is not to be evaluated by R. You can leave a note to yourself about what the next line is supposed to do, what its strengths and limitations are, or anything else you want to remember later. To leave a note, we use “comments”, which are a line of text that starts with the hash symbol #. Anything on a line after a # will be ignored by R.\r\n\r\n\r\n# This is a comment. Running this in R will have no effect.\r\n\r\n\r\nExecuting R script\r\nThe Run button in the Script Editor panel toolbar will run either the current line of code or any block of selected code. You can use your First script.R code to gain familiarity with this functionality.\r\nPlace the cursor anywhere in line 3 of your script [x = 34]. Now press the Run button in the Script Editor panel toolbar. Three things happen: 1) the code is transferred to the command console, 2) the code is executed, and 3) the cursor moves to the next line in your script. Press the Run button three more times. RStudio executes lines 4, 5, and 6 of your script.\r\n4.2 Using rmarkdown\r\nLearn how to construct an RMarkdown file, please visit the site.\r\n4.3 Objects and operators\r\nR objects\r\nIn RStudio, if you want to create a object called x and give it a value of 4 using the symbol “<-”, we would write:\r\n\r\n\r\nx <- 4\r\nx\r\n\r\n[1] 4\r\n\r\nThe middle “<-” tells R to assign the value on the right to the object on the left. After running the command above, when running x in a command, it would be replaced by its value 4. If adding 3 to x, the expect would get 7.\r\n\r\n\r\nx + 3\r\n\r\n[1] 7\r\n\r\nObject in R can store more than just simple numbers. It can store lists of numbers, functions, graphics, etc., depending on what values get assigned to the object.\r\nYou can always reassign a new value to a object. If telling R that x is equal to 32:\r\n\r\n\r\nx <- 32\r\n\r\n\r\nthen x takes its new value:\r\n\r\n\r\nx\r\n\r\n[1] 32\r\n\r\nNaming objects and functions in R is pretty flexible. A name has to start with a letter, but that can be followed by letters or numbers. Names in R are case-sensitive, which means that Weights and weights are completely different things to R. A good idea is to give object names be as descriptive as possible, so that you will know what you meant later on when looking at it. Sometimes clear naming means that it is best to have multiple words in the name, but you can’t have spaces. A common approach is to chain the words with underscores, as in weights_before_hospital.\r\nR operators\r\nAn operator is a symbol that tells the compiler to perform specific mathematical or logical manipulations. R language is rich in built-in operators and provides following types of operators.\r\nArithmetic Operators: +, -, *, /\r\nRelational Operators: >, <, =, !=, etc\r\nLogical Operators: TRUE, FALSE\r\nAssignment Operators: <-\r\nMiscellaneous Operators: %in%, :, %*%\r\n\r\n\r\n\r\n",
      "last_modified": "2023-10-14T23:01:49+08:00"
    },
    {
      "path": "homework_finalpresent.html",
      "title": "Homework assignments and Final presentation",
      "description": "Homework assignments and Final presentation are two important parts of the class, each of which will give you 30% of the overall credit if you finish them all and submit them on time!\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Homework assignments\r\n1.1 Preparing assignments\r\n1.2 Rules for evaluation\r\n\r\n2. Final presentation\r\n2.1 Suggested topics\r\n2.2 How will we do it\r\n2.3 Rules for presentation\r\n\r\n3. Rules for evaluation\r\n\r\n1. Homework assignments\r\n1.1 Preparing assignments\r\nThere will be a total of 9-10 homework assignments during the semester, each counting for ~ 3% of the total evaluation.\r\nAlways be created and saved an R script as a *.R file, which is reproducible (see here what reproducible means)\r\nAlways be included your name (Chinese) and student ID, which is inserted as the first line of the R script like this:\r\n\r\n\r\n# Ecology Homework x, Author: your_name, Student ID: xxxxxxxx\r\n\r\n\r\nUsually be uploaded the R script into your github before the deadline. Otherwise, 50% of the grade is going down.\r\n1.2 Rules for evaluation\r\nEach homework is evaluated on a scale between 0 to 100% in the following way:\r\nSubmit the homework on time, and it is correct: 100%, but it has some flaws, we will minus the following:\r\nmissing header line: -10%\r\ngarbage code present, e.g. names (data), View (Df): -10%\r\nscript saved as a wrong file type: -20%\r\nscript not reproducible: -10%\r\nscript not clean and tidy: -10% (see here what clean and tidy script means)\r\n\r\nSubmit the homework late, but is correct but late: 50%\r\nSubmit the homework on time, but it is wrong: 30%\r\nNot submit the homework at all: 0%\r\n2. Final presentation\r\n2.1 Suggested topics\r\nFree to choose what topic you want to present, considering that it fits the following criteria:\r\nthe topic is related to your interest/study\r\nyou solved it in R\r\nit is reasonably simple that everyone in the class can have at least a rough idea of what is going on\r\n2.2 How will we do it\r\nLast two classes will be focused on these presentations and discussions. Half of the students will present the first and half the second class\r\nEach presentation is very brief with exactly four slides (saved this talk in pdf)\r\nAfter the section with talks, we will start a personal discussion. At that moment, you will be either presenter or reviewer. Each presenter will be evaluated by teachers and several other reviewers\r\nEach presenter will sit with her/his computer at the table with an empty chair beside it, and reviewers will come for discussion. Each reviewer will have assigned presenters she/he has to visit, plus can visit some others and discuss.\r\nAs a presenter, you need to present your project and then be available for discussion\r\nAs a reviewer, you need to be present at the class for talks and for personal review\r\n2.3 Rules for presentation\r\nPrepare a presentation with exactly four slides:\r\nSlide 1: Opening slide with the title of your presentation, your name (Chinese), your major and the year of the study\r\nSlide 2: Brief introductory slide - what’s going on, what kind of data and what kind of problem/method\r\nSlide 3: Results - what came out? (figures, numbers, or something else)\r\nSlide 4: the highlights of the R script used for the presentation. The highlights may include important libraries and functions used in the script or important sections of the R code. Do not copy the whole R code!\r\nPresentation slides, presentations and R code annotation should be in English\r\nThe presentation should take 3 minutes\r\nPlease check the pdf file; please, make sure you upload your slides and R code before the deadline!\r\nR code should be fully reproducible, but it should be as tidy as possible.\r\nNote that all presentations and all R codes will be made available to all students within the class!\r\n3. Rules for evaluation\r\nThe evaluation will consist of two equal parts: evaluation by classmates and evaluation by me. As a reviewer, you need to evaluate four criteria (presentation, idea, whether they understand their R code, and whether the R code is correct, tidy and clean, see below)\r\nRules for evaluation (max 30% of overall class score):\r\nitems\r\ncriteria\r\nscores\r\nPresentation\r\nWhether to relate, and whether to attract the attention. If not, leave blank.\r\n6%\r\nIdea\r\nHow interesting is the idea, in your opinion?\r\n6%\r\nR code\r\nWhether and how the R code works, if clearly, is ok\r\n6%\r\nR code correct, tidy & clean?\r\nCan the script run, and is it reproducible? Is it free of mistakes?\r\n6%\r\nReviewing others\r\nDoing and returning the review of 5 presenters assigned to you as reviewers\r\n6%\r\nTotal\r\n\r\n30%\r\n\r\n\r\n\r\n",
      "last_modified": "2023-10-14T23:01:50+08:00"
    },
    {
      "path": "index.html",
      "title": "Data-Driven Ecology",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Home\r\n          \r\n          \r\n          About the Class\r\n          Getting Started\r\n          \r\n          \r\n          Lessons\r\n           \r\n          ▾\r\n          \r\n          \r\n          Lesson 1: General overview\r\n          Lesson 2: Data manipulation\r\n          Data and Models\r\n          \r\n          \r\n          Blog\r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Data-Driven Ecology\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                         GitHub\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                         Email\r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            \r\n            Welcome!\r\n            \r\n            \r\n            \r\n            \r\n            This class walks participants through the steps required\r\n            to use R for a wide array of data analysis relevant to\r\n            research in biology and ecology.\r\n            The core skills we will cover are as follows:\r\n            best practices in data retriever, management, and\r\n            visualization\r\n            basic knowledge about data analysis with machine\r\n            learning methods\r\n            beneficial practices in modeling complex patterns and\r\n            processes\r\n            The class is extremely participatory, where a lot of\r\n            training modules are delivered based on the learner’s\r\n            needs.\r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Data-Driven Ecology\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                       GitHub\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                       Email\r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              \r\n              Welcome!\r\n              \r\n              \r\n              \r\n              \r\n              This class walks participants through the steps\r\n              required to use R for a wide array of data analysis\r\n              relevant to research in biology and ecology.\r\n              The core skills we will cover are as follows:\r\n              best practices in data retriever, management, and\r\n              visualization\r\n              basic knowledge about data analysis with machine\r\n              learning methods\r\n              beneficial practices in modeling complex patterns and\r\n              processes\r\n              The class is extremely participatory, where a lot of\r\n              training modules are delivered based on the learner’s\r\n              needs.\r\n              \r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2023-10-14T23:01:50+08:00"
    }
  ],
  "collections": []
}
