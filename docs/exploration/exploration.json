[
  {
    "path": "exploration/2023-10-15-datamaniplation/",
    "title": "Lesson 1: Data manipluation",
    "description": "In this lesson, you will learn what types of data and how to maniplate data, including data import and export, as well as simple statistic analysis.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Acquring data from R\r\n1.1 The data within R\r\n1.2 The data outside R\r\n\r\n2. Data types and manipulation\r\n2.1 Vectors and maniplation\r\n2.2 lists and manipluation\r\n2.3 Matrics and manipulation\r\n2.4 Arrays and manipulation\r\n2.5 Factors and manipulation\r\n2.6 Data frames and manipulation\r\n\r\n3. Data visualization in R\r\n3.1 Pie charts\r\n3.2 Bar charts\r\n3.3 Boxplots\r\n3.4 Histograms\r\n3.5 Line Charts\r\n3.6 Scatterplots\r\n\r\n\r\n1. Acquring data from R\r\n1.1 The data within R\r\nThere are several ways to find the included datasets in R:\r\nUsing data() to list the datasets of all loaded packages (not only the ones from the datasets package); the datasets are ordered by package\r\nUsing data(package = .packages(all.available = TRUE)) to list all datasets in the available packages on your computer (i.e. also the not-loaded ones)\r\nUsing data(package = “packagename”) to list the datasets built in the package, so data(package = “plyr”) will give the datasets in the plyr package\r\n1.2 The data outside R\r\nIn R you can read data from files stored outside the R environment. You can also write data into files which will be stored and accessed by computers. R can read and write into various file formats like csv, excel, xml etc.\r\nThe file should be present in current working directory so that R can read it. You can set our own directory and read files from there. You can use the getwd() function to check current directory, and also use setwd()functional to set a new working directory.\r\n\r\n[1] \"D:/education-website/_data_exploration/2023-10-15-datamaniplation\"\r\n\r\nA CSV or excel File\r\nThe csv file is a text file in which the values in the columns are separated by a comma. You can use read.csv() function to read it into R.\r\nMicrosoft Excel is the most widely used spreadsheet which stores data in the .xls or .xlsx format. R can read directly from the files using some specific packages, such as xlsx package.\r\n\r\n[1] FALSE\r\n\r\nFrom the web site\r\nMany websites can provide data. For example, WHO provides reports on health and medical information in the form of CSV, txt and XML files. Using R, we can programmatically extract data from websites. Some R packages, such as “RCurl”, “XML”, and “stringr”, are used to connect to the URL’s, identify required links for the data and download them to R environment.\r\n\r\n\r\n\r\nFor example, if you visit the URL weather data and download the CSV files using R for the year 2015.\r\n\r\n\r\n\r\nFrom the databases\r\nThe data is Relational database systems are stored in a normalized format. So, to carry out statistical computing you will need very advanced and complex Sql queries. But R can connect easily to many relational databases like MySql, Oracle, Sql server etc. and fetch records from them as a dataframe. Once the data is available in the R environment, it becomes a normal R data set and can be manipulated or analyzed using packages and functions. Below you will be using MySql as our reference database for connecting to R.\r\nOnce the RMySQL package is installed we create a connection object in R to connect to the database. It takes the username, password, database name and host name as input.\r\n\r\n\r\n\r\nYou can query the database tables in MySql using the dbSendQuery() function. The query executs in MySql and the result is returned using the fetch() function. Finally it is stored as a dataframe in R.\r\n\r\n\r\n\r\nYou can pass any valid select query to get the result.\r\n\r\n\r\n\r\nYou can update the rows in a Mysql table by passing the update query to the dbSendQuery() function.\r\n\r\n\r\n\r\nYou can create tables in the MySql using the dbWriteTable() function. It overwrites the table if it already exists and takes a dataframe as input.\r\n\r\n\r\n\r\nYou can drop the tables in MySql database passing the drop table statement into the dbSendQuery() in the same way as we used it for querying data from tables.\r\n\r\n\r\n\r\n2. Data types and manipulation\r\nGenerally, you may store information of data types like character, integer, floating point, and Boolean, etc. Based on the data type of a variable, the operating system allocates memory and decides what can be stored. There are many types of R-objects. The frequently used ones are:\r\nVectors\r\nLists\r\nMatrices\r\nArrays\r\nFactors\r\nData Frames\r\n2.1 Vectors and maniplation\r\nVectors\r\nIn R the very basic data types are the R-objects called vectors. When creating a vector with more than one element, you should use c() function which means to combine the elements into a vector.\r\n\r\n\r\n# Create a vector.\r\napple <- c('red','green',\"yellow\")\r\nprint(apple)\r\n\r\n[1] \"red\"    \"green\"  \"yellow\"\r\n\r\nVector manipulation\r\nTwo vectors of same length can be added, subtracted, multiplied or divided giving the result as a vector output.\r\n\r\n[1]  7 19  4 13  1 13\r\n[1] -1 -3  4 -3 -1  9\r\n[1] 12 88  0 40  0 22\r\n[1] 0.7500000 0.7272727       Inf 0.6250000 0.0000000 5.5000000\r\n\r\nElements in a vector can be sorted using the sort() function.\r\n\r\n[1]  -9   0   3   4   5   8  11 304\r\n\r\n\r\n[1] 304  11   8   5   4   3   0  -9\r\n\r\n2.2 lists and manipluation\r\nLists\r\nA list is an R-object which can contain many different types of elements inside it like vectors, functions and even another list inside it.\r\n\r\n\r\n# Create a list.\r\nlist1 <- list(c(2,5,3),21.3,sin)\r\n\r\n# Print the list.\r\nprint(list1)\r\n\r\n[[1]]\r\n[1] 2 5 3\r\n\r\n[[2]]\r\n[1] 21.3\r\n\r\n[[3]]\r\nfunction (x)  .Primitive(\"sin\")\r\n\r\nList manipulation\r\nA list can be converted to a vector so that the elements of the vector can be used for further manipulation. All the arithmetic operations on vectors can be applied after the list is converted into vectors. To do this conversion, we use the unlist() function. It takes the list as input and produces a vector.\r\n\r\n[[1]]\r\n[1] 1 2 3 4 5\r\n[[1]]\r\n[1] 10 11 12 13 14\r\n\r\n\r\n[1] 1 2 3 4 5\r\n[1] 10 11 12 13 14\r\n[1] 11 13 15 17 19\r\n\r\n2.3 Matrics and manipulation\r\nMatrices\r\nA matrix is a two-dimensional rectangular R-object. It can be created using a vector input to the matrix function.\r\n\r\n\r\n# Create a matrix.\r\nM = matrix(c('a','a','b','c','b','a'), nrow = 2, ncol = 3, byrow = TRUE)\r\nprint(M)\r\n\r\n     [,1] [,2] [,3]\r\n[1,] \"a\"  \"a\"  \"b\" \r\n[2,] \"c\"  \"b\"  \"a\" \r\n\r\nMatrix manipulation\r\nCreate a matrix taking a vector of numbers as input.\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    3    4    5\r\n[2,]    6    7    8\r\n[3,]    9   10   11\r\n[4,]   12   13   14\r\n     [,1] [,2] [,3]\r\n[1,]    3    7   11\r\n[2,]    4    8   12\r\n[3,]    5    9   13\r\n[4,]    6   10   14\r\n     col1 col2 col3\r\nrow1    3    4    5\r\nrow2    6    7    8\r\nrow3    9   10   11\r\nrow4   12   13   14\r\n\r\nVarious mathematical operations are performed on the matrices using the R operators. The result of the operation is also a matrix.\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    3   -1    2\r\n[2,]    9    4    6\r\n     [,1] [,2] [,3]\r\n[1,]    5    0    3\r\n[2,]    2    9    4\r\nResult of addition \r\n     [,1] [,2] [,3]\r\n[1,]    8   -1    5\r\n[2,]   11   13   10\r\nResult of subtraction \r\n     [,1] [,2] [,3]\r\n[1,]   -2   -1   -1\r\n[2,]    7   -5    2\r\n\r\n2.4 Arrays and manipulation\r\nArrays\r\nWhile matrices are confined to two dimensions, arrays can be of any number of dimensions. The array function takes a dim attribute which creates the required number of dimension. Below is an example array with two elements which are 3x3 matrices each. Arrays can store data in more than two dimensions.\r\n\r\n\r\n# Create an array.\r\na <- array(c('green','yellow'),dim = c(3,3,2)) #  2 matrices each with 3 rows and 3 columns\r\nprint(a)\r\n\r\n, , 1\r\n\r\n     [,1]     [,2]     [,3]    \r\n[1,] \"green\"  \"yellow\" \"green\" \r\n[2,] \"yellow\" \"green\"  \"yellow\"\r\n[3,] \"green\"  \"yellow\" \"green\" \r\n\r\n, , 2\r\n\r\n     [,1]     [,2]     [,3]    \r\n[1,] \"yellow\" \"green\"  \"yellow\"\r\n[2,] \"green\"  \"yellow\" \"green\" \r\n[3,] \"yellow\" \"green\"  \"yellow\"\r\n\r\nArray Manipulation\r\nArrays are the R-objects that can store data in more than two dimensions. If we create an array of dimension (2, 3, 4) then it creates 4 rectangular matrices each with 2 rows and 3 columns. Arrays can store only data type.\r\nAn array is created using the array() function. It takes vectors as input and uses the values in the dim parameter to create an array.\r\n\r\n, , 1\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    5   10   13\r\n[2,]    9   11   14\r\n[3,]    3   12   15\r\n\r\n, , 2\r\n\r\n     [,1] [,2] [,3]\r\n[1,]    5   10   13\r\n[2,]    9   11   14\r\n[3,]    3   12   15\r\n\r\n2.5 Factors and manipulation\r\nFactors\r\nFactors are the R-objects which are created using a vector. It stores a vector along with the distinct values of the elements in the vector as labels. The labels are always character irrespective of whether it is numeric or character or Boolean etc. in the input vector. They are useful in statistical modeling.\r\nFactors are created using the factor() function. The nlevels() functions gives the count of levels.\r\n\r\n\r\n# Create a vector\r\napple_colors <- c('green','green','yellow','red','red','red','green')\r\n\r\n# Create a factor object\r\nfactor_apple <- factor(apple_colors)\r\n\r\n# Print the factor\r\nprint(factor_apple)\r\n\r\n[1] green  green  yellow red    red    red    green \r\nLevels: green red yellow\r\n\r\nprint(nlevels(factor_apple))\r\n\r\n[1] 3\r\n\r\nFactor Manipulation\r\nFactors are the R-objects that are used to categorize the data and store it as levels. They can store both strings and integers. They are useful in the columns that have a limited number of unique values. Like “Male,”Female” and True, False etc. They are useful in data analysis for statistical modeling.\r\n\r\n [1] \"East\"  \"West\"  \"East\"  \"North\" \"North\" \"East\"  \"West\"  \"West\" \r\n [9] \"West\"  \"East\"  \"North\"\r\n[1] FALSE\r\n [1] East  West  East  North North East  West  West  West  East  North\r\nLevels: East North West\r\n[1] TRUE\r\n\r\n2.6 Data frames and manipulation\r\nData Frames\r\nData frames are tabular data objects. Unlike a matrix in data frame each column can contain different modes of data. The first column can be numeric while the second column can be character and third column can be logical. It is a list of vectors of equal length.\r\nData Frames are created using the data.frame() function.\r\n\r\n\r\n# Create the data frame.\r\nBMI <-   data.frame(\r\n   gender = c(\"Male\", \"Male\",\"Female\"), \r\n   height = c(152, 171.5, 165), \r\n   weight = c(81,93, 78),\r\n   Age = c(42,38,26)\r\n)\r\nprint(BMI)\r\n\r\n  gender height weight Age\r\n1   Male  152.0     81  42\r\n2   Male  171.5     93  38\r\n3 Female  165.0     78  26\r\n\r\nData frame Manipulation\r\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. You can use data.frame() function to create a dataframe.\r\n\r\n  emp_id emp_name salary start_date\r\n1      1     Rick 623.30 2012-01-01\r\n2      2      Dan 515.20 2013-09-23\r\n3      3 Michelle 611.00 2014-11-15\r\n4      4     Ryan 729.00 2014-05-11\r\n5      5     Gary 843.25 2015-03-27\r\n\r\nThe structure of the data frame can be seen by using str() function.\r\n\r\n'data.frame':   5 obs. of  4 variables:\r\n $ emp_id    : int  1 2 3 4 5\r\n $ emp_name  : chr  \"Rick\" \"Dan\" \"Michelle\" \"Ryan\" ...\r\n $ salary    : num  623 515 611 729 843\r\n $ start_date: Date, format: \"2012-01-01\" ...\r\n\r\nThe statistical summary and nature of the data can be obtained by applying summary() function.\r\n\r\n     emp_id    emp_name             salary        start_date        \r\n Min.   :1   Length:5           Min.   :515.2   Min.   :2012-01-01  \r\n 1st Qu.:2   Class :character   1st Qu.:611.0   1st Qu.:2013-09-23  \r\n Median :3   Mode  :character   Median :623.3   Median :2014-05-11  \r\n Mean   :3                      Mean   :664.4   Mean   :2014-01-14  \r\n 3rd Qu.:4                      3rd Qu.:729.0   3rd Qu.:2014-11-15  \r\n Max.   :5                      Max.   :843.2   Max.   :2015-03-27  \r\n\r\nExtract specific column from a dataframe using column name.\r\n\r\n  emp.data.emp_name emp.data.salary\r\n1              Rick          623.30\r\n2               Dan          515.20\r\n3          Michelle          611.00\r\n4              Ryan          729.00\r\n5              Gary          843.25\r\n\r\nExtract the first two rows and then all columns.\r\n\r\n  emp_id emp_name salary start_date\r\n1      1     Rick  623.3 2012-01-01\r\n2      2      Dan  515.2 2013-09-23\r\n\r\nExtract 3rd and 5th row with 2nd and 4th column.\r\n\r\n  emp_name start_date\r\n3 Michelle 2014-11-15\r\n5     Gary 2015-03-27\r\n\r\nA dataframe can be expanded by adding columns and rows.\r\n\r\n  emp_id emp_name salary start_date       dept\r\n1      1     Rick 623.30 2012-01-01         IT\r\n2      2      Dan 515.20 2013-09-23 Operations\r\n3      3 Michelle 611.00 2014-11-15         IT\r\n4      4     Ryan 729.00 2014-05-11         HR\r\n5      5     Gary 843.25 2015-03-27    Finance\r\n\r\nTo add more rows permanently to an existing data frame, we need to bring in the new rows in the same structure as the existing data frame and use the rbind() function.\r\n\r\n  emp_id emp_name salary start_date       dept\r\n1      1     Rick 623.30 2012-01-01         IT\r\n2      2      Dan 515.20 2013-09-23 Operations\r\n3      3 Michelle 611.00 2014-11-15         IT\r\n4      4     Ryan 729.00 2014-05-11         HR\r\n5      5     Gary 843.25 2015-03-27    Finance\r\n6      6    Rasmi 578.00 2013-05-21         IT\r\n7      7   Pranab 722.50 2013-07-30 Operations\r\n8      8    Tusar 632.80 2014-06-17   Fianance\r\n\r\nData reshape\r\nData Reshaping in R is about changing the way data is organized into rows and columns. Most of the time data processing in R is done by taking the input data as a dataframe. It is easy to extract data from the rows and columns of a data frame but there are situations when we need the dataframe in a format that is different from format in which we received it. R has many functions to split, merge and change the rows to columns and vice-versa in a data frame.\r\nYou can join multiple vectors to create a data frame using the cbind()function. Also we can merge two data frames using rbind() function.\r\n\r\n# # # # The First data frame\r\n     city       state zipcode\r\n[1,] \"Tampa\"    \"FL\"  \"33602\"\r\n[2,] \"Seattle\"  \"WA\"  \"98104\"\r\n[3,] \"Hartford\" \"CT\"  \"6161\" \r\n[4,] \"Denver\"   \"CO\"  \"80294\"\r\n# # # The Second data frame\r\n       city state zipcode\r\n1     Lowry    CO   80230\r\n2 Charlotte    FL   33949\r\n# # # The combined data frame\r\n       city state zipcode\r\n1     Tampa    FL   33602\r\n2   Seattle    WA   98104\r\n3  Hartford    CT    6161\r\n4    Denver    CO   80294\r\n5     Lowry    CO   80230\r\n6 Charlotte    FL   33949\r\n\r\nYou can merge two dataframes by using the merge() function. The data frames must have same column names on which the merging happens.\r\nIn the example below, you consider the data sets about Diabetes in the MASS package. we merge the two data sets based on the values of blood pressure (“bp”) and body mass index (“bmi”). To choose these two columns for merging, the records where values of two variables match in both data sets are combined together to form a single data frame.\r\n\r\n   bp  bmi npreg.x glu.x skin.x ped.x age.x type.x npreg.y glu.y\r\n1  60 33.8       1   117     23 0.466    27     No       2   125\r\n2  64 29.7       2    75     24 0.370    33     No       2   100\r\n3  64 31.2       5   189     33 0.583    29    Yes       3   158\r\n4  64 33.2       4   117     27 0.230    24     No       1    96\r\n5  66 38.1       3   115     39 0.150    28     No       1   114\r\n6  68 38.5       2   100     25 0.324    26     No       7   129\r\n7  70 27.4       1   116     28 0.204    21     No       0   124\r\n8  70 33.1       4    91     32 0.446    22     No       9   123\r\n9  70 35.4       9   124     33 0.282    34     No       6   134\r\n10 72 25.6       1   157     21 0.123    24     No       4    99\r\n11 72 37.7       5    95     33 0.370    27     No       6   103\r\n12 74 25.9       9   134     33 0.460    81     No       8   126\r\n13 74 25.9       1    95     21 0.673    36     No       8   126\r\n14 78 27.6       5    88     30 0.258    37     No       6   125\r\n15 78 27.6      10   122     31 0.512    45     No       6   125\r\n16 78 39.4       2   112     50 0.175    24     No       4   112\r\n17 88 34.5       1   117     24 0.403    40    Yes       4   127\r\n   skin.y ped.y age.y type.y\r\n1      20 0.088    31     No\r\n2      23 0.368    21     No\r\n3      13 0.295    24     No\r\n4      27 0.289    21     No\r\n5      36 0.289    21     No\r\n6      49 0.439    43    Yes\r\n7      20 0.254    36    Yes\r\n8      44 0.374    40     No\r\n9      23 0.542    29    Yes\r\n10     17 0.294    28     No\r\n11     32 0.324    55     No\r\n12     38 0.162    39     No\r\n13     38 0.162    39     No\r\n14     31 0.565    49    Yes\r\n15     31 0.565    49    Yes\r\n16     40 0.236    38     No\r\n17     11 0.598    28     No\r\n[1] 17\r\n\r\nOne of the most interesting aspects of R is about changing the shape of the data in multiple steps to get a desired shape. The functions used to do this are called melt() and cast(). We consider the dataset called ships in the MASS package.\r\n\r\n\r\n\r\nYou can cast the molten data into a new form where the aggregate of each type of ship for each year is created. It is done using the cast() function.\r\n3. Data visualization in R\r\n3.1 Pie charts\r\nIn R the pie chart is created using the pie() function which takes positive numbers as a vector input.\r\n\r\n\r\n\r\n3.2 Bar charts\r\nThe below script will create and save the bar chart in the current R working directory.\r\n\r\n\r\n\r\n3.3 Boxplots\r\nBoxplots are created in R by using the boxplot() function.\r\n\r\n\r\n\r\n3.4 Histograms\r\nR creates histogram using hist() function. This function takes a vector as an input and uses some more parameters to plot histograms.\r\n\r\n\r\n\r\n3.5 Line Charts\r\nA line chart is a graph that connects a series of points by drawing line segments between them. The plot() function is used to create the line graph.\r\n\r\n\r\n\r\n3.6 Scatterplots\r\nScatterplots show many points plotted in the Cartesian plane. Each point represents the values of two variables. One variable is chosen in the horizontal axis and another in the vertical axis. The simple scatterplot is created using the plot() function.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-15T18:04:50+08:00",
    "input_file": {}
  },
  {
    "path": "exploration/2023-10-15-datavisualization/",
    "title": "Lesson 2: Data visualization",
    "description": "This section helps you create the most popular visualizations - from quick and dirty plots to publication-ready graphs. The text relies heavily on the ggplot2 package for graphics, but other approaches covered as well.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nLoad the required packages\r\n\r\nThis dataset contains four lists: env, fish, xy, species. Of these, the fish file lists all species that were recorded in the Doubs River. The data can be extracted as:\r\n\r\n\r\ndata(doubs, package = \"ade4\")\r\nDoubsSpe <- doubs$fish\r\nDoubsEnv <- doubs$env\r\nDoubsSpa <- doubs$xy\r\n\r\n\r\nLoad the required packages\r\nlibrary(vegan)\r\nlibrary(labdsv)\r\nlibrary(MASS)\r\nlibrary(mvpart)\r\nlibrary(ggplot2)\r\nIn ecological data, it’s common that a site or plot has 0’s value for any organisms. If so, you should remove it out of your dataset, and do not include the site for your analysis. As for the data of doubs, you can detect 0’s value like this:\r\n\r\n\r\nrowSums(DoubsSpe) == 0\r\n\r\n    1     2     3     4     5     6     7     8     9    10    11 \r\nFALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE \r\n   12    13    14    15    16    17    18    19    20    21    22 \r\nFALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE \r\n   23    24    25    26    27    28    29    30 \r\nFALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE \r\n\r\n#colSums(DoubsSpe) == 0\r\n\r\n\r\nBecause the site no. 8 contains no species, so we remove the row 8 and corresponding abiotic data (site 8), and assign the data new variables:\r\n\r\n\r\nspe <- DoubsSpe[-8, ]\r\nenv <- DoubsEnv[-8, ]\r\n\r\n\r\nExplore the environment dataset and calculate correlation efficients among variables. See the website for details.\r\n\r\n\r\n\r\nThe hclust() for hierarchical clustering order is used to reorder the correlation matrix according to the correlation coefficient. This is useful to identify the hidden pattern in the matrix.\r\n\r\n\r\n\r\nReordered correlation data:\r\n\r\n\r\n# Reorder the correlation matrix\r\ncormat <- reorder_cormat(cormat)\r\n\r\n\r\nThe correlation matrix has redundant information. We’ll use the codes below to set half of it to NA.\r\n\r\n\r\n# Get upper triangle of the correlation matrix\r\nget_upper_tri <- function(cormat){\r\n    cormat[lower.tri(cormat)] <- NA\r\n    return(cormat)\r\n  }\r\nupper_tri <- get_upper_tri(cormat)\r\n#upper_tri\r\n\r\n\r\nMelt the correlation data, and drop the rows with NA values:\r\n\r\n\r\n# Melt the correlation matrix and drop NA\r\nlibrary(reshape2) #The package reshape used to melt the correlation matrix\r\nmelted_cormat <- melt(upper_tri, na.rm = TRUE) # na.rm-->drop NA\r\n#head(melted_cormat)\r\n\r\n\r\nAdd mort details to plot for improving the figure and perform the codes below.\r\n\r\n\r\n# Create a heatmap\r\nlibrary(ggplot2)\r\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +\r\n   geom_tile(color = \"white\") + \r\n  # using geom_title() to create heetmap, \"color\" means the grid line color\r\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \r\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \r\n    name=\"Pearson\\nCorrelation\") + \r\n  # use scale_fill_gradient2 () to creates a color gradient (low-mid-high), here bule notes egative correlations and red notes positive correlations. limit = c(-1,1) as correlation coefficients range from -1 to 1. See legend for details.\r\n  theme_minimal() + # Create a white background with no grid lines and no border\r\n  theme(axis.text.x = element_text(angle = 0, vjust = 1, \r\n    size = 10, hjust = 1)) + # x axis tick mark labels \r\n coord_fixed() # ensures one unit on the x-axis is the same length as one unit on the y-axis\r\n\r\n# Print the heatmap\r\nprint(ggheatmap)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "exploration/2023-10-15-datavisualization/distill-preview.png",
    "last_modified": "2023-10-15T19:31:50+08:00",
    "input_file": {}
  },
  {
    "path": "exploration/2023-10-15-exploratoryanalysis/",
    "title": "Lesson 3: Exploratory Analysis",
    "description": "Data exploration is an important step in data science. It mainly includes traditional statistical analysis to interpret data patterns, to find stories and to gain instights.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Raw data and preprocessing\r\n1.1 Structures\r\n1.2 Preprocessing\r\n\r\n2. Descriptive statistics\r\n2.1 Species data distribution\r\n2.2 Species data standardization\r\n2.3 Species data transformations\r\n2.4 Environmental data VS. collinearity\r\n\r\n3. Preliminary analysis\r\n3.1 Association measures\r\n3.2 Clustering\r\n\r\n4. Advance analysis\r\n4.1 General introduction\r\n4.2 Unconstrained Ordination\r\n\r\n\r\nThis section is adopted from the module, the tutorial and the book.\r\n1. Raw data and preprocessing\r\n1.1 Structures\r\nA community sample (plot, sample, etc), represent presence/absence or quantity (count, cover or biomass) of each species in each sample. Three matrices are used to delineate these data, i.e., the matrix of species composition (L matrix, sample × species), the matrix of sample attributes (R matrix, sample × sample attributes, with environmental variables, ), and the matrix of species attributes, like species traits or species indicator values (Q matrix, species × species attributes).\r\n\r\n1.2 Preprocessing\r\nAfter the data have been imported into R, it is useful to explore data first, check for missing values and outliers, check for range and type of species and environmental data, apply transformation or standardization if necessary, check for possible correlations between environmental variables etc.\r\nMissing and 0 values\r\nMissing data are elements in the matrix with no value, in R usually replaced by NA (not available). Note that there is an important difference between 0 and NA (e.g. species was not recorded and gets zero cover or abundance). For example, a pH-meter got broken and didn’t measure pH in some samples, you should not replace the value by 0, since it does not mean that the pH of that sample is so low.\r\nAs for the samples with missing values, if there are lots of missing values scattered across different variables, the analysis will be based on rather few samples. One way to reduce this effect is to remove the variables with the highest proportion of missing values from the analysis. Another option is to replace the missing values by estimates if these could be reasonably accurate (mostly by interpolation, e.g. from similar plots, neighbours, values measured at the same time somewhere close, or values predicted by a model).\r\nis.na() will work on individual values, vectors, lists, and data frames. It will return TRUE or FALSE where you have an NA.\r\n\r\n\r\ndf <- read.csv('missd_exam.csv')\r\n# df\r\n# is.na(df)\r\nnames(df) # the column names\r\n\r\n[1] \"example\" \"data\"    \"set\"    \r\n\r\nsum(is.na(df)) # How many NAs in my data frame\r\n\r\n[1] 3\r\n\r\nYou can use sum() and which() to figure out where NAs locate, and finally remote them, like this:\r\n\r\n\r\n#which(is.na(df))\r\nwhich(is.na(df$data)) # Which row contains an NA in the 'data' column\r\n\r\n[1] 4\r\n\r\n# na.omit(df) # remove the rows with NAs\r\n\r\n\r\nAs for the 0’s values, you should delete them if they affect results. Take the doubs dataset as an example. This data set gives environmental variables, fish species and spatial coordinates for 30 sites.doubs is a list with 4 components.\r\n\r\n\r\ndata(doubs, package = \"ade4\") # load data from ade4 package\r\n# class(doubs) # find the object types\r\n# names(doubs) # check names of the list\r\n\r\n\r\nThen read the species, environment, distribution data from the list:\r\n\r\n\r\n#Species and environment data from doubs\r\nspecies <- doubs$fish\r\n# head(spe)\r\n# which(is.na(species)) # check missing values in a data set\r\n# rowSums(species) == 0 # check the site without any species\r\nspe <- species[-8,] # the site 8 without species is removed\r\n\r\nenvironment <- doubs$env\r\nenv <- environment[-8,] # remove corresponding abiotic data for the site 8 \r\n\r\n\r\nOutliers\r\nOutliers are those values within a given variable that are conspicuously different from other values. Outlier value could get quite influential in the analysis, so it is worth to treat it in advance. First, spending a reasonable time to ensure that such value is not a mistype. If the sample really describes conditions that are rather different from the rest of the data set, it may be reasonable to remove them, since there may not be enough replications to describe this difference or phenomena.\r\nThere is a number of ways how to detect outliers. A simple exploratory data analysis (EDA) could reveal it graphically, e.g. using a box plot or a histogram. In a box plot, the outlier is defined as a value 1.5 times of interquartile range above upper quartile (Q3) or below lower quartile (Q1); the interquartile range is the range between upper and lower quartile: IQR = Q3-Q1.\r\n\r\nVisual approaches such as histogram, scatter plot (such as Q-Q plot), and boxplot are the easiest method to detect outliers. Let’s take an example of the univariate dataset and identify outliers using visual approaches.\r\n\r\n\r\n# x <- c(5, 8, 8, 12, 14, 15, 16, 19, 20, 22, 24, 25, 25, 26, 30, 48)\r\n# boxplot(x) # show the outliers\r\n\r\n\r\nYou can also use boxplot() to remove the outliers, like this:\r\n\r\n\r\n# boxplot(x, outline=FALSE) # remove the outliers\r\n\r\n\r\nUsing the interquartile range (IQR) to detect the data points which ranks at 25th percentile (first quartile or Q1) and 75th percentile (third quartile or Q3) in the dataset (IQR = Q3 - Q1), and futher detect outliers in three steps:\r\nFirst, calculating Q1 and Q3 with summary():\r\n\r\n\r\n# get values of Q1, Q3, and IQR\r\n# summary(x)\r\n#   Min.  1st Qu.  Median    Mean   3rd Qu.    Max. \r\n#   5.00   13.50   19.50    19.81   25.00     48.00 \r\n\r\n\r\nThen getting IQR with IQR() to calculate the threshold:\r\n\r\n\r\n\r\nFinally detecting the outliers and remove them:\r\n\r\n\r\n# find outlier\r\n# x[which(x < Tmin | x > Tmax)]\r\n# [1] 48\r\n# remove outlier\r\n# x[which(x > Tmin & x < Tmax)]\r\n\r\n\r\nHomework assignment: detecting the outliers of the doubs dataset following the above procedure.\r\nData transformation\r\nTransforming data is needed because statistical analyses and tests require that the residuals are normally distributed and have homogeneous variance, or because linear relationships may be easy to interpret. A good indicator of whether data need to be transformed is projecting the values using the histograms and checking whether the distribution is symmetrical, right-skewed or left-skewed. Ecological data are often right-skewed because they are limited by zero at the beginning. Several transformation ways are as follows:\r\nLog transformation is suitable for strongly right-skewed data\r\n\r\n\r\n# y <- log10(x) # for positively skewed data,\r\n# y <- log10(max(x+1) - x) # for negatively skewed data\r\n\r\n\r\n\r\nSquare-root transformation is suitable for slightly right-skewed data.\r\n\r\n\r\n# y <- sqrt(x) # for positively skewed data,\r\n# y <- sqrt(max(x+1) - x) # for negatively skewed data\r\n\r\n\r\nPower transformation is suitable for left-skewed data.\r\nReciprocal transformation is suitable for ratios (e.g. height/weight body ratio).\r\n\r\n\r\n# y <- 1/x # for positively skewed data\r\n# y <- 1/(max(x+1) - x) # for negatively skewed data\r\n\r\n\r\n2. Descriptive statistics\r\n2.1 Species data distribution\r\nTake the doubs datasets for example, illustrate how to do exploratory data analysis. First, we analysed species distribution.\r\n\r\n\r\n# fish species names\r\n#names(spe)\r\n\r\n# all species distribution\r\nab <- table(unlist(spe)) # if want to see, put (the entire code line) in a bracket\r\nbarplot(ab, las=1, # flips labels on y-axis into horizontal position\r\n        xlab=\"Abundance class\", ylab=\"Frequency\", col=grey(5:0/5))\r\n\r\n\r\n# individual species distribution\r\n# ggplot(spe, aes(x = Cogo)) + geom_histogram()\r\n# get the data\r\n# cogo <- table(spe$Cogo)\r\n# barplot(cogo, las=1, xlab=\"Abundance class\", ylab=\"Frequency\", col=grey(5:0/5))\r\n\r\n# Can see that an intermediate number of sites contain the highest number of species.\r\n# spe.pres <- colSums(spe > 0) # the number of sites where each species is present. \r\n# hist(spe.pres, main=\"Species occurrence\", las=1, xlab=\"Frequency of occurrences\", # breaks=seq(0,30, by=5), col=\"grey\")\r\n\r\n# Calculate the number of species that occur at each site\r\n#site.pres <- rowSums(spe>0) #number of species with a value greater than 0 in that site row\r\n#hist(site.pres, main=\"Species richness\", las=1, xlab=\"Frequency of sites\", ylab=\"Number of species\", breaks=seq(0,30, by=5), col=\"grey\")\r\n\r\n\r\n2.2 Species data standardization\r\nStandardization changes the data using a statistic calculated from data itself, e.g. mean, range, the sum of values (it is data-dependent). The most common reason to apply standardization is to remove differences in relative weights (importance) of individual variables or samples.\r\nCentring: Standardised variable has mean equal to zero.\r\nz-scores: Standardised variable has mean equal to zero and standard deviation equal to one.\r\n\r\n\r\n# creating Standardization function\r\n#standardize = function(x){\r\n#  z <- (x - mean(x)) / sd(x)\r\n#  return( z)\r\n#}\r\n  \r\n# apply your function to the dataset\r\n#dataframe[2:3] <-\r\n#  apply(dataframe[2:3], 2, standardize)\r\n  \r\n#displaying result\r\n#dataframe\r\n\r\n\r\nRanging: Changes the range of variable, e.g. into [0, 1].\r\n2.3 Species data transformations\r\nSometimes species/community data may also need to be standardized or transformed. The decostand() function in vegan provides standardization and transformation options for community composition data.\r\n\r\n\r\n#Transforming abundance or count data to pres-abs data\r\nspe.pa<-vegan::decostand(spe, method=\"pa\") \r\n\r\n#Hellinger transformation\r\nspe.hel<-vegan::decostand(spe, method=\"hellinger\") #can also use method=”hell”\r\n \r\n#Chi-square transformation\r\nspe.chi<-vegan::decostand(spe, method=\"chi.square\")\r\n\r\n\r\n2.4 Environmental data VS. collinearity\r\n\r\n\r\n# names(env)\r\n# dim(env)\r\n# str(env)\r\n# head(env)\r\n# summary(env)\r\npairs(env, main=\"Bivariate Plots of the Environmental Data\" ) \r\n\r\n\r\n\r\nIn this case, the environmental data (explanatory variables) are all in different units and need to be standardized prior to computing distance measures to perform ordination analyses. Standardize the environmental data (11 variables) using the function decostand() in vegan.\r\n\r\n\r\n\r\n3. Preliminary analysis\r\n3.1 Association measures\r\nPrior to starting multivariate analyses, you have matrices with ecological data (such as the DoubsEnv or DoubsSpe), and use them to create association matrices between objects or among descriptors. Exploring the possible association measures can help you to understand what distance measure to use within ordination methods.\r\nDistance measures of species data\r\nWe can use the vegdist() function to compute dissimilarity indices in order to quantifying community composition data. These can then be visualized as a matrix if desired.\r\n\r\n\r\n\r\nIn the spe.db matrix, the numbers represent the distance (dissimilarity) between the first 3 species in DoubsSpe would look like this:\r\n\r\n[1] \"matrix\" \"array\" \r\n          Cogo      Satr      Phph\r\nCogo 0.0000000 0.6000000 0.6842105\r\nSatr 0.6000000 0.0000000 0.1428571\r\nPhph 0.6842105 0.1428571 0.0000000\r\n\r\nYou can see that when comparing a species to itself (e.g. Cogo to Cogo), the distance = 0, because species 1 is like itself. You can create graphical depictions of these association matrices using the coldiss() function of the gclus package.\r\n\r\n\r\n library(gclus)\r\n source(\"coldiss.R\")\r\n# coldiss(spe.db, byrank=FALSE, diag=TRUE) # Heat map of Bray-Curtis dissimilarity\r\n# coldiss(spe.dj, byrank=FALSE, diag=TRUE) # Heat map of Jaccard dissimilarity\r\n# coldiss(spe.dg, byrank=FALSE, diag=TRUE) # Heat map of Gower dissimilarity\r\n\r\n\r\nDistance measures of env data\r\nLet’s look at associations between environmental variables (also known as Q mode)\r\n\r\n\r\n\r\nWe can then look at the dependence between environmental variables (also known as R mode):\r\n\r\n\r\n#(env.pearson<-cor(env)) # Pearson r linear correlation\r\n#round(env.pearson, 2) #Rounds the coefficients to 2 decimal points \r\n#(env.ken<-cor(env, method=\"kendall\")) # Kendall tau rank correlation\r\n#round(env.ken, 2) \r\n\r\n\r\nThe Pearson correlation measures the linear correlation between two variables. The Kendall tau is a rank correlation which means that it quantifies the relationship between two descriptors or variables when the data are ordered within each variable.\r\nIn some cases, there may be mixed types of environmental variables. Q mode can still be used to find associations between these environmental variables. We’ll do this by first creating an example dataframe:\r\n3.2 Clustering\r\nOne application of association matrices is clustering. It is not a statistical method per se, because it does not test a hypothesis, but it highlights structures in the data by partitioning the objects or the descriptors. As a result, similar objects are combined into groups. One goal of ecologists could be to divide a set of sites into groups with respect to their environmental conditions or their community composition.\r\nThere are several families of clustering methods. Let’s compare the single and complete linkage clustering methods using the Doubs fish species data.\r\n\r\n\r\nspe.dhel <- vegdist(spe.hel, method=\"euclidean\") #generates the distance matrix from Hellinger transformed data\r\n \r\nhead(spe.dhel)# Hellinger distances among sites\r\n\r\n[1] 0.8420247 0.9391305 1.0616631 1.2308244 1.1153793 0.9391305\r\n\r\n#Perform single linkage clustering\r\nspe.dhel.single<-hclust(spe.dhel, method=\"single\")\r\nplot(spe.dhel.single)\r\n\r\n\r\n#Perform complete linkage clustering\r\nspe.dhel.complete<-hclust(spe.dhel, method=\"complete\")\r\nplot(spe.dhel.complete)\r\n\r\n\r\n\r\nIn order to compare this dendrogram to the single and complete linkage clustering results, one must calculate the square root of the distances.\r\n\r\n\r\n#Perform Ward minimum variance clustering\r\nspe.dhel.ward<-hclust(spe.dhel, method=\"ward.D2\")\r\nplot(spe.dhel.ward)\r\n\r\n\r\n#Re-plot the dendrogram by using the square roots of the fusion levels\r\nspe.dhel.ward$height<-sqrt(spe.dhel.ward$height)\r\nplot(spe.dhel.ward)\r\n\r\n\r\nplot(spe.dhel.ward, hang=-1) # hang=-1 aligns all objets on the same line\r\n\r\n\r\n\r\nOne must be careful in the choice of an association measure and clustering method in order to correctly address a problem. What are you most interested in: gradients? Contrasts? In addition, the results should be interpreted with respect to the properties of the method used. If more than one method seems suitable to an ecological question, computing them all and compare the results would be to go. As a reminder, clustering is not a statistical method, but further steps can be taken to identify interpretable clusters, or to compute clustering statistics.\r\n4. Advance analysis\r\n4.1 General introduction\r\nOrdination is to reduce multidimensional information stored in community data into a few imaginable, interpretable and printable dimensions. We use it either to describe community pattern (usually the purpose of unconstrained = indirect ordination) or to explain changes in species composition by some (e.g. environmental, spatial, temporal) variables (constrained = direct ordination).\r\nOrdination methods can be divided according to two criteria: whether their algorithm includes environmental variables along to the species composition data (unconstrained ordination methods do not, while constrained do), and what type of species composition data is used for analysis (raw data of sample-species matrix of species composition, pre-transformed data using Hellinger transformation, or distance matrix (sample-sample symmetric matrix of distances between samples).\r\n\r\nRaw data-based\r\n\r\nTransformation-based\r\nDistance-based\r\n\r\nLinear\r\nUnimodal\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUnconstrained\r\nPCA\r\nCA & DCA\r\ntb-PCA\r\nPCoA, NMDS\r\nConstrained\r\nRDA\r\nCCA\r\ntb-RDA\r\ndb-RDA\r\nThe schemas below show the three alternative approaches you can use for the ordination of community ecology data, for either unconstrained or constrained ordination. You can decide to analyze data by either a) PCA/CA (depending on whether community composition data are homogeneous or heterogeneous), b) transformation-based PCA (first pre-transforming species composition data via Hellinger standardization, and then using PCA; doesn’t matter whether community composition data are homogeneous or heterogeneous), or c) distance-based PCoA or NMDS. But it often does not make much sense to combine these approaches.\r\n4.2 Unconstrained Ordination\r\nUnconstrained ordination allows us to organize samples, sites or species along continuous gradients (e.g. ecological or environmental). The key difference between unconstrained and constrained ordination is that in the unconstrained techniques we are not attempting to define a relationship between independent and dependent sets of variables.\r\nUnconstrained ordination can be used to:\r\nAssess relationships within a set of variables (not between sets).\r\nFind key components of variation between samples, sites, species etc.\r\nReduce the number dimensions in multivariate data without substantial loss of information.\r\nCreate new variables for use in subsequent analyses (such as regression). These principal components are weighted, linear combinations of the original variables in the ordination.\r\nPrincipal Component Analysis\r\nPrincipal component analysis (PCA) is used to generate a few key variables from a larger set of variables that still represent as much of the variation in the dataset as possible. PCA is powerful to analyze quantitative descriptors (such as species abundances), but can not be applied to binary data (such as species absence/presence). PCA preserves Euclidean distances and detects linear relationships. As a consequence, raw species abundance data are subjected to a pre-transformation (i.e. a Hellinger transformation) before computing a PCA.\r\nTo do a PCA you need:\r\nA set of variables (with no distinction between independent or dependent variables, i.e. a set of species OR a set of environmental variables).\r\nSamples that are measured for the same set of variables.\r\nGenerally a dataset that is longer than it is wide is preferred.\r\nThe “spe” data includes 27 fish taxa. To simplify the 27 fish taxa into a smaller number of fish-related variables or to identify where different sites or samples are associated with particular fish taxa we can run a PCA. Run a PCA on Hellinger-transformed species data:\r\n\r\n\r\n\r\nThe eigenvalue is the value of the change in the length of a vector. It is the amount of variation explained by each axis in a PCA. From the summary, you can see how much of the variance in the data is explained by the unconstrained variables. In this case, the total variance of the sites explained by the species is 0.5. The summary also tells you what proportion of the total explained variance is explained by each principal component in the PCA: the first axis of the PCA thus explains 51.33% of the variation, and the second axis 12.78%.\r\nSometimes you may want to extract the scores (i.e. the coordinates within a PCA biplot) for either the “sites” (the rows in your dataset, whether they be actual sites or not) or the “species” (the variables in your data, whether they be actual species or some other variables). This is useful if you want to then use a principal component as a variable in another analysis, or to make additional graphics. For example, with the “spe” dataset, you might want to obtain a single variable that is a composite of all the fish abundance data and then use that use that variable in a regression with another variable, or plot across a spatial gradient. To extract scores from a PCA, use the scores() function:\r\n\r\n\r\n\r\nThe PCA on the “spe” fish data produces as many principal components as there are fish taxon (columns), which in this case means that 27 principal components are produced. In many cases though, you may have done a PCA to reduce the number of variables to deal with and produce composite variables for the fish. In this case, you are likely interested in knowing how many of these principal components are actually significant or adding new information to the PCA (i.e. how many principal components do you need to retain before you aren’t really explaining any more variance with the additional principal components). To determine this, you can use the Kaiser-Guttman criterion and produce a barplot showing at what point the principal components are no longer explaining significant amount of variance. The code for the barplot below shows the point at which the variance explained by a new principal component explains less than the average amount explained by all of the eigenvalues:\r\n\r\n       PC1        PC2        PC3        PC4        PC5 \r\n0.25796049 0.06424089 0.04632294 0.03850244 0.02196526 \r\n\r\n\r\nFrom this barplot, you can see that once you reach PC6, the proportion of variance explained falls below the average proportion explained by the other components. If you take another look at the PCA summary, you will notice that by the time you reach PC5, the cumulative proportion of variance explained by the principal components is 85%.\r\nA PCA is not just for species data. It can also be run and interpreted in the same way using standardized environmental variables:\r\n\r\n\r\nCall:\r\nrda(X = env.z) \r\n\r\nPartitioning of variance:\r\n              Inertia Proportion\r\nTotal              11          1\r\nUnconstrained      11          1\r\n\r\nEigenvalues, and their contribution to the variance \r\n\r\nImportance of components:\r\n                        PC1    PC2     PC3     PC4     PC5     PC6\r\nEigenvalue            6.446 2.2252 0.99825 0.39831 0.36224 0.25361\r\nProportion Explained  0.586 0.2023 0.09075 0.03621 0.03293 0.02306\r\nCumulative Proportion 0.586 0.7883 0.87903 0.91524 0.94817 0.97123\r\n                          PC7     PC8      PC9     PC10     PC11\r\nEigenvalue            0.16106 0.11062 0.022949 0.017476 0.004378\r\nProportion Explained  0.01464 0.01006 0.002086 0.001589 0.000398\r\nCumulative Proportion 0.98587 0.99593 0.998013 0.999602 1.000000\r\n\r\nScaling 2 for species and site scores\r\n* Species are scaled proportional to eigenvalues\r\n* Sites are unscaled: weighted dispersion equal on all dimensions\r\n* General scaling constant of scores:  4.189264 \r\n\r\n\r\nSpecies scores\r\n\r\n         PC1     PC2      PC3       PC4       PC5      PC6\r\ndfs  1.10786  0.4901 -0.20230  0.045871  0.187500 -0.20053\r\nalt -1.06604 -0.5607  0.15135  0.193929 -0.096582 -0.05541\r\nslo -0.95746 -0.5295 -0.25933 -0.352365 -0.066943 -0.39476\r\nflo  0.98732  0.6262 -0.23731  0.009137  0.105599 -0.31209\r\npH  -0.02679  0.4991  1.14319 -0.014864  0.005398 -0.17594\r\nhar  0.91493  0.5514 -0.09538 -0.128923 -0.639664  0.07145\r\npho  1.02238 -0.6481  0.20016 -0.178923 -0.040772 -0.04615\r\nnit  1.14057 -0.1628  0.05081 -0.283125  0.272940  0.20434\r\namm  0.96776 -0.7382  0.18740 -0.198493  0.021140  0.04733\r\noxy -0.99282  0.4993  0.04744 -0.543295  0.012373  0.06953\r\nbdo  0.96051 -0.7274  0.09630  0.089149 -0.178489 -0.14521\r\n\r\n\r\nSite scores (weighted sums of species scores)\r\n\r\n        PC1     PC2       PC3      PC4     PC5        PC6\r\n1  -1.35309 -1.0614 -0.626184 -1.14841  1.0494 -1.821e+00\r\n2  -1.05499 -0.7841  0.195705  0.90757  1.7294  2.655e-01\r\n3  -0.97457 -0.4890  1.340010  0.61152  0.8592 -7.288e-01\r\n4  -0.90281 -0.3118  0.000372  0.17712 -0.2052  5.288e-01\r\n5  -0.45666 -0.6973  0.550276  1.16964 -1.2500  1.273e-01\r\n6  -0.81070 -0.7590 -0.318284  0.77249  0.2537  1.263e-01\r\n7  -0.85277 -0.1858  0.231151 -0.37159 -1.3564 -3.616e-01\r\n9  -0.27926 -0.4610  0.061754  1.60909 -1.2127  8.870e-01\r\n10 -0.59145 -0.5554 -1.595293 -0.35281 -0.6397 -2.559e-01\r\n11 -0.34078  0.3167  0.005014 -1.23777 -0.7883 -2.345e-01\r\n12 -0.44165  0.3209 -0.697214 -0.46903 -0.3928  5.963e-01\r\n13 -0.39855  0.6314  0.003511 -0.90415 -1.0778  5.002e-05\r\n14 -0.22649  0.7350  0.946755 -0.87823 -0.8730  3.814e-02\r\n15 -0.21927  1.0432  2.269502 -0.19576  0.1024 -2.360e-01\r\n16 -0.16778  0.2507 -0.340084 -0.54136  0.1054  6.195e-01\r\n17  0.14914  0.3628 -0.171537 -0.14337  0.1372  1.435e+00\r\n18  0.08633  0.3674 -0.238531 -0.44014  0.2901  1.015e+00\r\n19  0.10967  0.4821  0.232435 -0.28363  0.7316  9.414e-01\r\n20  0.18575  0.3732 -0.268777 -0.69333  0.9729  1.131e+00\r\n21  0.16766  0.3112 -0.834583  0.21603  0.7147  5.100e-01\r\n22  0.13041  0.4842 -0.108135  0.18812  0.2895 -5.716e-01\r\n23  1.28519 -1.3164  0.715652 -0.57204 -0.6499 -1.021e+00\r\n24  1.01542 -0.4735  0.019519  1.43289 -0.5981  1.440e-01\r\n25  2.10059 -2.1406  0.361157 -1.21146  0.1786  4.424e-01\r\n26  0.89379 -0.1213 -0.671652  0.86581  0.3046 -8.871e-02\r\n27  0.61092  0.3178 -0.139402  0.31511  0.8178 -9.684e-01\r\n28  0.82353  0.8569  0.802337 -0.04239  0.8747  5.103e-02\r\n29  0.67793  1.0652 -1.729365  0.28387 -0.2944 -1.028e+00\r\n30  0.83450  1.4380  0.003890  0.93621 -0.0730 -1.543e+00\r\n\r\nCall:\r\nrda(X = env.z) \r\n\r\nPartitioning of variance:\r\n              Inertia Proportion\r\nTotal              11          1\r\nUnconstrained      11          1\r\n\r\nEigenvalues, and their contribution to the variance \r\n\r\nImportance of components:\r\n                        PC1    PC2     PC3     PC4     PC5     PC6\r\nEigenvalue            6.446 2.2252 0.99825 0.39831 0.36224 0.25361\r\nProportion Explained  0.586 0.2023 0.09075 0.03621 0.03293 0.02306\r\nCumulative Proportion 0.586 0.7883 0.87903 0.91524 0.94817 0.97123\r\n                          PC7     PC8      PC9     PC10     PC11\r\nEigenvalue            0.16106 0.11062 0.022949 0.017476 0.004378\r\nProportion Explained  0.01464 0.01006 0.002086 0.001589 0.000398\r\nCumulative Proportion 0.98587 0.99593 0.998013 0.999602 1.000000\r\n\r\nScaling 2 for species and site scores\r\n* Species are scaled proportional to eigenvalues\r\n* Sites are unscaled: weighted dispersion equal on all dimensions\r\n* General scaling constant of scores:  4.189264 \r\n\r\n\r\nSpecies scores\r\n\r\n         PC1     PC2      PC3       PC4       PC5      PC6\r\ndfs  1.10786  0.4901 -0.20230  0.045871  0.187500 -0.20053\r\nalt -1.06604 -0.5607  0.15135  0.193929 -0.096582 -0.05541\r\nslo -0.95746 -0.5295 -0.25933 -0.352365 -0.066943 -0.39476\r\nflo  0.98732  0.6262 -0.23731  0.009137  0.105599 -0.31209\r\npH  -0.02679  0.4991  1.14319 -0.014864  0.005398 -0.17594\r\nhar  0.91493  0.5514 -0.09538 -0.128923 -0.639664  0.07145\r\npho  1.02238 -0.6481  0.20016 -0.178923 -0.040772 -0.04615\r\nnit  1.14057 -0.1628  0.05081 -0.283125  0.272940  0.20434\r\namm  0.96776 -0.7382  0.18740 -0.198493  0.021140  0.04733\r\noxy -0.99282  0.4993  0.04744 -0.543295  0.012373  0.06953\r\nbdo  0.96051 -0.7274  0.09630  0.089149 -0.178489 -0.14521\r\n\r\n\r\nSite scores (weighted sums of species scores)\r\n\r\n        PC1     PC2       PC3      PC4     PC5        PC6\r\n1  -1.35309 -1.0614 -0.626184 -1.14841  1.0494 -1.821e+00\r\n2  -1.05499 -0.7841  0.195705  0.90757  1.7294  2.655e-01\r\n3  -0.97457 -0.4890  1.340010  0.61152  0.8592 -7.288e-01\r\n4  -0.90281 -0.3118  0.000372  0.17712 -0.2052  5.288e-01\r\n5  -0.45666 -0.6973  0.550276  1.16964 -1.2500  1.273e-01\r\n6  -0.81070 -0.7590 -0.318284  0.77249  0.2537  1.263e-01\r\n7  -0.85277 -0.1858  0.231151 -0.37159 -1.3564 -3.616e-01\r\n9  -0.27926 -0.4610  0.061754  1.60909 -1.2127  8.870e-01\r\n10 -0.59145 -0.5554 -1.595293 -0.35281 -0.6397 -2.559e-01\r\n11 -0.34078  0.3167  0.005014 -1.23777 -0.7883 -2.345e-01\r\n12 -0.44165  0.3209 -0.697214 -0.46903 -0.3928  5.963e-01\r\n13 -0.39855  0.6314  0.003511 -0.90415 -1.0778  5.002e-05\r\n14 -0.22649  0.7350  0.946755 -0.87823 -0.8730  3.814e-02\r\n15 -0.21927  1.0432  2.269502 -0.19576  0.1024 -2.360e-01\r\n16 -0.16778  0.2507 -0.340084 -0.54136  0.1054  6.195e-01\r\n17  0.14914  0.3628 -0.171537 -0.14337  0.1372  1.435e+00\r\n18  0.08633  0.3674 -0.238531 -0.44014  0.2901  1.015e+00\r\n19  0.10967  0.4821  0.232435 -0.28363  0.7316  9.414e-01\r\n20  0.18575  0.3732 -0.268777 -0.69333  0.9729  1.131e+00\r\n21  0.16766  0.3112 -0.834583  0.21603  0.7147  5.100e-01\r\n22  0.13041  0.4842 -0.108135  0.18812  0.2895 -5.716e-01\r\n23  1.28519 -1.3164  0.715652 -0.57204 -0.6499 -1.021e+00\r\n24  1.01542 -0.4735  0.019519  1.43289 -0.5981  1.440e-01\r\n25  2.10059 -2.1406  0.361157 -1.21146  0.1786  4.424e-01\r\n26  0.89379 -0.1213 -0.671652  0.86581  0.3046 -8.871e-02\r\n27  0.61092  0.3178 -0.139402  0.31511  0.8178 -9.684e-01\r\n28  0.82353  0.8569  0.802337 -0.04239  0.8747  5.103e-02\r\n29  0.67793  1.0652 -1.729365  0.28387 -0.2944 -1.028e+00\r\n30  0.83450  1.4380  0.003890  0.93621 -0.0730 -1.543e+00\r\n\r\nScaling refers to what portion of the PCA is scaled to the eigenvalues. Scaling = 2 means that the species scores are scaled by eigenvalues, whereas scaling = 1 means that site scores are scaled by eigenvalues. Scaling = 3 means that both species and site scores are scaled symmetrically by square root of eigenvalues. Using scaling = 1 means that the Euclidean distances among objects (e.g. the rows in your data) are preserved, whereas scaling = 2 means that the correlations among descriptors (e.g. the columns in this data) are preserved. This means that when you look at a biplot of a PCA that has been run with scaling=2, the angle between descriptors represents correlation.\r\n\r\n     PC1      PC2 \r\n6.445919 2.225186 \r\n\r\n\r\nAs you saw in the explanation of the summary output, a lot of information can be extracted from a PCA before even plotting it. A PCA figure is the best way to convey major patterns. A PCA biplot includes the x-axis as the first Principal Component and the y-axis as the second Principal Component. A basic biplot without any customization could be plotted like this, where the site positions are shown by black numbers and species’ positions are shown by red species codes. Remember, species positions come from plotting species along PCs and the site positions are derived from the weighted sums of species scores.\r\n\r\n\r\n\r\nWhat conclusions can you draw from this plot? You can see that there are only a few sites that are farther away from the majority. The species names are shown by their names in red and from the plot, you can see for example that the species “ABL” is not found or not found in the same prevalence in the majority of sites as other species closer to the centre of the ordination.\r\nNow let’s look at a plot of the environmental PCA:\r\n\r\n\r\n\r\nNonmetric MultiDimensional Scaling\r\nThe unconstrained ordination allow to organize objects (e.g. sites) characterized by descriptors (e.g. species) in full-dimensional space. In other words, PCA, CA and PCoA computes lots of ordination axes representing the variation of species among sites and preserve distance among objects (the Euclidean distances in PCA, the Chi2 distances in CA and the other distances in PCoA). Users can then select the axis of interest to represent objects in an ordination plot. The produced biplot represents the distance among objects (e.g. the between-sites similarity), but fails to represent the whole variation dimensions of the ordination space.\r\nIn some case, the priority is not to preserve the exact distances among sites, but rather to represent as accurately as possible the relationships among objects in a number of axes (generally two or three). In such cases, nonmetric multidimensional scaling (NMDS) is the solution. A biplot of NMDS is better to represent similarity between objects: dissimilar objects are apart in the ordination space and similar objects close to one another. Also, NMDS allows users to choose the distance measure applied to calculate the ordination.\r\nTo find the best object representation, NMDS applies an iterative procedure to position the objects in the number of dimensions to minimize a stress function (scaled from 0 to 1). Consequently, the lower the stress value, the better the representation of objects in the ordination-space is. An additional way to assess the appropriateness of an NDMS is to construct a Shepard diagram which plot distances among objects in the ordination plot against the original distances. The R2 obtained from the regression between these two distances measure the goodness-of-fit of the NMDS ordination.\r\n\r\nRun 0 stress 0.0747782 \r\nRun 1 stress 0.1152149 \r\nRun 2 stress 0.07478399 \r\n... Procrustes: rmse 0.003625267  max resid 0.0143458 \r\nRun 3 stress 0.08843919 \r\nRun 4 stress 0.07477801 \r\n... New best solution\r\n... Procrustes: rmse 0.0002980715  max resid 0.001431492 \r\n... Similar to previous best\r\nRun 5 stress 0.1196716 \r\nRun 6 stress 0.1124391 \r\nRun 7 stress 0.1141839 \r\nRun 8 stress 0.09157455 \r\nRun 9 stress 0.08801543 \r\nRun 10 stress 0.08841667 \r\nRun 11 stress 0.08695588 \r\nRun 12 stress 0.07376241 \r\n... New best solution\r\n... Procrustes: rmse 0.01940148  max resid 0.09468274 \r\nRun 13 stress 0.122613 \r\nRun 14 stress 0.07477812 \r\nRun 15 stress 0.08917037 \r\nRun 16 stress 0.1223245 \r\nRun 17 stress 0.07383674 \r\n... Procrustes: rmse 0.003819841  max resid 0.01488639 \r\nRun 18 stress 0.1204797 \r\nRun 19 stress 0.08901468 \r\nRun 20 stress 0.08927164 \r\n*** Best solution was not repeated -- monoMDS stopping criteria:\r\n     1: no. of iterations >= maxit\r\n    19: stress ratio > sratmax\r\n[1] 0.07376241\r\n\r\n\r\nThe Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance (\\(R^2 > 0.95\\)), highlighting a high goodness-of-fit of the NMDS.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "exploration/2023-10-15-exploratoryanalysis/box2-1.png",
    "last_modified": "2023-10-15T20:23:42+08:00",
    "input_file": {}
  }
]
